{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "written-mattress",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "from dist3 import create_task\n",
    "#from esbn_pytorch import ESBN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36e8bfd5-9f55-4891-9eb6-b4bdbb342cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import log\n",
    "from modules import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64341df8-307a-4b22-a0cb-ec9f08153381",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_conv(nn.Module):\n",
    "\t#def __init__(self, args):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(Encoder_conv, self).__init__()\n",
    "\t\tlog.info('Building convolutional encoder...')\n",
    "\t\t# Convolutional layers\n",
    "\t\tlog.info('Conv layers...')\n",
    "\t\t'''\n",
    "\t\tself.conv1 = nn.Conv2d(1, 32, 4, stride=2, padding=1)\n",
    "\t\tself.conv2 = nn.Conv2d(32, 32, 4, stride=2, padding=1)\n",
    "\t\tself.conv3 = nn.Conv2d(32, 32, 4, stride=2, padding=1)\n",
    "\t\t'''\n",
    "\t\tself.conv1 = nn.Conv2d(1, 160, 4, stride=2, padding=1)\n",
    "\t\tself.conv2 = nn.Conv2d(160, 160, 4, stride=2, padding=1)\n",
    "\t\tself.conv3 = nn.Conv2d(160, 160, 4, stride=2, padding=1)\n",
    "\t\t# Fully-connected layers\n",
    "\t\tlog.info('FC layers...')\n",
    "\t\t#self.fc1 = nn.Linear(4*4*32, 256)\n",
    "\t\t#self.fc2 = nn.Linear(256, 128)\n",
    "\t\tself.fc1 = nn.Linear(64000, 256)\n",
    "\t\tself.fc2 = nn.Linear(256, 128)\n",
    "\t\t# Nonlinearities\n",
    "\t\tself.relu = nn.ReLU()\n",
    "\t\t# Initialize parameters\n",
    "\t\tfor name, param in self.named_parameters():\n",
    "\t\t\t# Initialize all biases to 0\n",
    "\t\t\tif 'bias' in name:\n",
    "\t\t\t\tnn.init.constant_(param, 0.0)\n",
    "\t\t\t# Initialize all pre-ReLU weights using Kaiming normal distribution\n",
    "\t\t\telif 'weight' in name:\n",
    "\t\t\t\tnn.init.kaiming_normal_(param, nonlinearity='relu')\n",
    "\tdef forward(self, x):\n",
    "\t\t# Convolutional layers\n",
    "\t\tconv1_out = self.relu(self.conv1(x))\n",
    "\t\tconv2_out = self.relu(self.conv2(conv1_out))\n",
    "\t\tconv3_out = self.relu(self.conv3(conv2_out))\n",
    "\t\t# Flatten output of conv. net\n",
    "\t\tconv3_out_flat = torch.flatten(conv3_out, 1)\n",
    "\t\t# Fully-connected layers\n",
    "\t\t#print(\"conv3_out_flat.size() =\",conv3_out_flat.size())\n",
    "\t\t#time.sleep(120)\n",
    "\t\tfc1_out = self.relu(self.fc1(conv3_out_flat))\n",
    "\t\tfc2_out = self.relu(self.fc2(fc1_out))\n",
    "\t\t# Output\n",
    "\t\tz = fc2_out\n",
    "\t\treturn z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "brutal-rebel",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\t#def __init__(self, task_gen, args):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(Model, self).__init__()\n",
    "\t\t# Encoder\n",
    "\t\tlog.info('Building encoder...')\n",
    "\t\t'''\n",
    "\t\tif args.encoder == 'conv':\n",
    "\t\t\tself.encoder = Encoder_conv(args)\n",
    "\t\telif args.encoder == 'mlp':\n",
    "\t\t\tself.encoder = Encoder_mlp(args)\n",
    "\t\telif args.encoder == 'rand':\n",
    "\t\t\tself.encoder = Encoder_rand(args)\n",
    "\t\t'''\n",
    "\t\tself.encoder = Encoder_conv()# removed \"args\" argument\n",
    "\t\t# LSTM and output layers\n",
    "\t\tlog.info('Building LSTM and output layers...')\n",
    "\t\tself.z_size = 128\n",
    "\t\tself.key_size = 256\n",
    "\t\tself.hidden_size = 512\n",
    "\t\tself.lstm = nn.LSTM(self.key_size + 1, self.hidden_size, batch_first=True)\n",
    "\t\tself.key_w_out = nn.Linear(self.hidden_size, self.key_size)\n",
    "\t\tself.g_out = nn.Linear(self.hidden_size, 1)\n",
    "\t\tself.confidence_gain = nn.Parameter(torch.ones(1))\n",
    "\t\tself.confidence_bias = nn.Parameter(torch.zeros(1))\n",
    "\t\t#self.y_out = nn.Linear(self.hidden_size, task_gen.y_dim)\n",
    "\t\ty_out = 8 # number of outputs/ ESBN = 4\n",
    "\t\tself.y_out = nn.Linear(self.hidden_size, y_out)\n",
    "\t\t# Context normalization\n",
    "\t\t#if args.norm_type == 'contextnorm' or args.norm_type == 'tasksegmented_contextnorm':\n",
    "\t\tif True: # assumes \"contextnorm or tasksegmented_contextnorm\"\n",
    "\t\t\tself.contextnorm = True\n",
    "\t\t\tself.gamma = nn.Parameter(torch.ones(self.z_size))\n",
    "\t\t\tself.beta = nn.Parameter(torch.zeros(self.z_size))\n",
    "\t\telse:\n",
    "\t\t\tself.contextnorm = False\n",
    "\t\t'''\n",
    "\t\tif args.norm_type == 'tasksegmented_contextnorm':\n",
    "\t\t\tself.task_seg = task_gen.task_seg\n",
    "\t\telse:\n",
    "\t\t\tself.task_seg = [np.arange(task_gen.seq_len)]\n",
    "\t\t'''\n",
    "\t\tseq_len = 16 # number of images per Raven problem / ESBN = 9        \n",
    "\t\tself.task_seg = [np.arange(seq_len)]\n",
    "\t\t# Nonlinearities\n",
    "\t\tself.relu = nn.ReLU()\n",
    "\t\tself.sigmoid = nn.Sigmoid()\n",
    "\t\tself.softmax = nn.Softmax(dim=1)\n",
    "\t\t# Initialize parameters\n",
    "\t\tfor name, param in self.named_parameters():\n",
    "\t\t\t# Encoder parameters have already been initialized\n",
    "\t\t\tif not ('encoder' in name) and not ('confidence' in name):\n",
    "\t\t\t\t# Initialize all biases to 0\n",
    "\t\t\t\tif 'bias' in name:\n",
    "\t\t\t\t\tnn.init.constant_(param, 0.0)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tif 'lstm' in name:\n",
    "\t\t\t\t\t\t# Initialize gate weights (followed by sigmoid) using Xavier normal distribution\n",
    "\t\t\t\t\t\tnn.init.xavier_normal_(param[:self.hidden_size*2,:])\n",
    "\t\t\t\t\t\tnn.init.xavier_normal_(param[self.hidden_size*3:,:])\n",
    "\t\t\t\t\t\t# Initialize input->hidden and hidden->hidden weights (followed by tanh) using Xavier normal distribution with gain = \n",
    "\t\t\t\t\t\tnn.init.xavier_normal_(param[self.hidden_size*2:self.hidden_size*3,:], gain=5.0/3.0)\n",
    "\t\t\t\t\telif 'key_w' in name:\n",
    "\t\t\t\t\t\t# Initialize weights for key output layer (followed by ReLU) using Kaiming normal distribution\n",
    "\t\t\t\t\t\tnn.init.kaiming_normal_(param, nonlinearity='relu')\n",
    "\t\t\t\t\telif 'g_out' in name:\n",
    "\t\t\t\t\t\t# Initialize weights for gate output layer (followed by sigmoid) using Xavier normal distribution\n",
    "\t\t\t\t\t\tnn.init.xavier_normal_(param)\n",
    "\t\t\t\t\telif 'y_out' in name:\n",
    "\t\t\t\t\t\t# Initialize weights for multiple-choice output layer (followed by softmax) using Xavier normal distribution\n",
    "\t\t\t\t\t\tnn.init.xavier_normal_(param)\n",
    "\tdef forward(self, x_seq, device):\n",
    "\t\t# Encode all images in sequence\n",
    "\t\tz_seq = []\n",
    "\t\tfor t in range(x_seq.shape[1]):\n",
    "\t\t\tx_t = x_seq[:,t,:,:].unsqueeze(1)\n",
    "\t\t\tz_t = self.encoder(x_t)\n",
    "\t\t\tz_seq.append(z_t)\n",
    "\t\tz_seq = torch.stack(z_seq, dim=1)\n",
    "\t\tif self.contextnorm:\n",
    "\t\t\tz_seq_all_seg = []\n",
    "\t\t\tfor seg in range(len(self.task_seg)):\n",
    "\t\t\t\tz_seq_all_seg.append(self.apply_context_norm(z_seq[:,self.task_seg[seg],:]))\n",
    "\t\t\tz_seq = torch.cat(z_seq_all_seg, dim=1)\n",
    "\t\t# Initialize hidden state\n",
    "\t\thidden = torch.zeros(1, x_seq.shape[0], self.hidden_size).to(device)\n",
    "\t\tcell_state = torch.zeros(1, x_seq.shape[0], self.hidden_size).to(device)\n",
    "\t\t# Initialize retrieved key vector\n",
    "\t\tkey_r = torch.zeros(x_seq.shape[0], 1, self.key_size + 1).to(device)\n",
    "\t\t# Memory model (extra time step to process key retrieved on final time step)\n",
    "\t\tfor t in range(x_seq.shape[1] + 1):\n",
    "\t\t\t# Image embedding\n",
    "\t\t\tif t == x_seq.shape[1]:\n",
    "\t\t\t\tz_t = torch.zeros(x_seq.shape[0], 1, self.z_size).to(device)\n",
    "\t\t\telse:\n",
    "\t\t\t\tz_t = z_seq[:,t,:].unsqueeze(1)\n",
    "\t\t\t# Controller\n",
    "\t\t\t# LSTM\n",
    "\t\t\tlstm_out, (hidden, cell_state) = self.lstm(key_r, (hidden, cell_state))\n",
    "\t\t\t# Key output layers\n",
    "\t\t\tkey_w = self.relu(self.key_w_out(lstm_out))\n",
    "\t\t\t# Gates\n",
    "\t\t\tg = self.sigmoid(self.g_out(lstm_out))\n",
    "\t\t\t# Task output layer\n",
    "\t\t\ty_pred_linear = self.y_out(lstm_out).squeeze()\n",
    "\t\t\ty_pred = y_pred_linear.argmax(1)\n",
    "\t\t\t# Read from memory\n",
    "\t\t\tif t == 0:\n",
    "\t\t\t\tkey_r = torch.zeros(x_seq.shape[0], 1, self.key_size + 1).to(device)\n",
    "\t\t\telse:\n",
    "\t\t\t\t# Read key\n",
    "\t\t\t\tw_k = self.softmax((z_t * M_v).sum(dim=2))\n",
    "\t\t\t\tc_k = self.sigmoid(((z_t * M_v).sum(dim=2) * self.confidence_gain) + self.confidence_bias)\n",
    "\t\t\t\tkey_r = g * (torch.cat([M_k, c_k.unsqueeze(2)], dim=2) * w_k.unsqueeze(2)).sum(1).unsqueeze(1)\n",
    "\t\t\t# Write to memory\n",
    "\t\t\tif t == 0:\n",
    "\t\t\t\tM_k = key_w\n",
    "\t\t\t\tM_v = z_t\n",
    "\t\t\telse:\n",
    "\t\t\t\tM_k = torch.cat([M_k, key_w], dim=1)\n",
    "\t\t\t\tM_v = torch.cat([M_v, z_t], dim=1)\n",
    "\t\treturn y_pred_linear, y_pred\n",
    "\tdef apply_context_norm(self, z_seq):\n",
    "\t\teps = 1e-8\n",
    "\t\tz_mu = z_seq.mean(1)\n",
    "\t\tz_sigma = (z_seq.var(1) + eps).sqrt()\n",
    "\t\tz_seq = (z_seq - z_mu.unsqueeze(1)) / z_sigma.unsqueeze(1)\n",
    "\t\tz_seq = (z_seq * self.gamma) + self.beta\n",
    "\t\treturn z_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "piano-inspection",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_npz_img(img_path):\n",
    "    data = np.load(img_path)\n",
    "    img = data['image']\n",
    "    target = data['target']\n",
    "    x = img[:,:,:]\n",
    "    #x = np.expand_dims(x, axis=0)\n",
    "    #x = x.reshape((x.shape[0],x.shape[1],x.shape[2],x.shape[3],1))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "standing-arthur",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_npz_target(target_path):\n",
    "    data = np.load(target_path)\n",
    "    target = data['target']\n",
    "    y = int(target)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "motivated-dream",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(folder,num_imgs,config):\n",
    "    X = []\n",
    "    #X = np.array(X)\n",
    "    Y =[]\n",
    "    #for i in range(num_imgs):\n",
    "    name = ''\n",
    "    if config=='train':\n",
    "        name = 'train'\n",
    "    elif config=='validate':\n",
    "        name = 'val'\n",
    "    else:\n",
    "        name = 'test'\n",
    "        \n",
    "    count = 0\n",
    "    i = 0\n",
    "    while count < num_imgs:\n",
    "        try:\n",
    "            x = grab_npz_img('/home/asw3x/RAVEN-10000/'+folder+'/RAVEN_%d_%s.npz'%(i,name))\n",
    "            y = grab_npz_target('/home/asw3x/RAVEN-10000/'+folder+'/RAVEN_%d_%s.npz'%(i,name))\n",
    "            i += 1\n",
    "        except:\n",
    "            i += 1\n",
    "            continue\n",
    "        X.append(x)\n",
    "        #X = np.concatenate(x)\n",
    "        Y.append(y)\n",
    "        count += 1\n",
    "    X = np.array(X)\n",
    "    X = np.squeeze(X)\n",
    "    #X = np.expand_dims(X, axis=4)\n",
    "    #X = X.reshape((X.shape[0],X.shape[2],X.shape[3],X.shape[1]))\n",
    "    #X = np.moveaxis(X, 1, -1)\n",
    "    return X,np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "stainless-impact",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_task(size=1000, task=\"center_single\"):\n",
    "    folder = task\n",
    "    train_size = size\n",
    "    test_size = train_size*0.4\n",
    "    \n",
    "    X_train, Y_train = create_dataset(folder,train_size,\"train\")\n",
    "    X_test, Y_test = create_dataset(folder,test_size,\"test\")\n",
    "    \n",
    "    # Create training and test sets\n",
    "    train_set = {'img_seq': X_train, 'y': Y_train}\n",
    "    test_set = {'img_seq': X_test, 'y': Y_test}\n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "surrounded-cooperation",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set,test_set = create_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "entertaining-acceptance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 16, 160, 160)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set['img_seq'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "photographic-actress",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set['y'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac478d0-80fd-48de-bf2c-ed81d333c813",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "structural-response",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None, target_transform=None):\n",
    "        '''\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        '''\n",
    "        self.img_seq = dataset['img_seq']\n",
    "        self.y = dataset['y']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        sample = {\"image\": image, \"label\": label}\n",
    "        '''\n",
    "        img_seq = self.img_seq[idx,:,:,:]\n",
    "        y = self.y[idx]\n",
    "        \n",
    "        return img_seq, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "moving-purpose",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = CustomImageDataset(training_set)\n",
    "train_dataloader = DataLoader(training_set, batch_size=32, shuffle=True)\n",
    "test_set = CustomImageDataset(test_set)\n",
    "test_dataloader = DataLoader(test_set, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "obvious-robinson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([32, 16, 160, 160])\n",
      "Labels batch shape: torch.Size([32])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 4\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0,0,:,:].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "excessive-loading",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([32, 16, 160, 160])\n",
      "Labels batch shape: torch.Size([32])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 6\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0,0,:,:].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "romantic-feelings",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef train_loop(train_dataloader, model, loss_fn, optimizer):\\n    size = len(dataloader.dataset)\\n    for batch, (X, y) in enumerate(dataloader):\\n        # Compute prediction and loss\\n        pred = model(X)\\n        loss = loss_fn(pred, y)\\n\\n        # Backpropagation\\n        optimizer.zero_grad()\\n        loss.backward()\\n        optimizer.step()\\n\\n        if batch % 128 == 0:\\n            loss, current = loss.item(), batch * len(X)\\n            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def train_loop(train_dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 128 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c779a5f-ebda-4176-bfe5-8ec303bd230a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(device, epoch, train_loader, model, optimizer):\n",
    "\tmodel.train()\n",
    "\t# Iterate over batches\n",
    "\tfor batch_idx, (X, y) in enumerate(train_loader):\n",
    "\t\t# Batch start time\n",
    "\t\tstart_time = time.time()\n",
    "\t\t# Use sequence indices to slice corresponding images\n",
    "\t\t#x_seq = all_imgs[seq_ind,:,:]\n",
    "\t\tx_seq = X.float()       \n",
    "\t\t# Load data to device\n",
    "\t\tx_seq = x_seq.to(device)\n",
    "\t\t#y = torch.nn.functional.one_hot(y,num_classes=8)\n",
    "\t\tprint(\"y =\",y)\n",
    "\t\ty = y.to(device)\n",
    "\t\t# Zero out gradients for optimizer \n",
    "\t\toptimizer.zero_grad()\n",
    "\t\t# Run model \n",
    "\t\t'''\n",
    "\t\tif 'MNM' in args.model_name:\n",
    "\t\t\ty_pred_linear, y_pred, const_loss = model(x_seq, device)\n",
    "\t\telse:\n",
    "\t\t\ty_pred_linear, y_pred = model(x_seq, device)\n",
    "\t\t'''\n",
    "\t\ty_pred_linear, y_pred = model(x_seq, device)\n",
    "\t\t# Loss\n",
    "\t\tloss_fn = nn.CrossEntropyLoss()\n",
    "\t\tloss = loss_fn(y_pred_linear, y)\n",
    "\t\t'''\n",
    "\t\tif 'MNM' in args.model_name:\n",
    "\t\t\tloss += const_loss\n",
    "\t\t'''\n",
    "\t\t# Update model\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\t# Batch duration\n",
    "\t\tend_time = time.time()\n",
    "\t\tbatch_dur = end_time - start_time\n",
    "\t\t# Report prgoress\n",
    "\t\t#if batch_idx % args.log_interval == 0:\n",
    "\t\tif batch_idx % 50 == 0:\n",
    "\t\t\t# Accuracy\n",
    "\t\t\tacc = torch.eq(y_pred, y).float().mean().item() * 100.0\n",
    "\t\t\t# Report \t\n",
    "\t\t\tlog.info('[Epoch: ' + str(epoch) + '] ' + \\\n",
    "\t\t\t\t\t '[Batch: ' + str(batch_idx) + ' of ' + str(len(train_loader)) + '] ' + \\\n",
    "\t\t\t\t\t '[Loss = ' + '{:.4f}'.format(loss.item()) + '] ' + \\\n",
    "\t\t\t\t\t '[Accuracy = ' + '{:.2f}'.format(acc) + '] ' + \\\n",
    "\t\t\t\t\t '[' + '{:.3f}'.format(batch_dur) + ' sec/batch]')\n",
    "\t\t\t# Save progress to file\n",
    "\t\t\t'''\n",
    "\t\t\ttrain_prog_f.write(str(batch_idx) + ' ' +\\\n",
    "\t\t\t\t\t\t\t   '{:.4f}'.format(loss.item()) + ' ' + \\\n",
    "\t\t\t\t\t\t\t   '{:.2f}'.format(acc) + '\\n')\n",
    "\t\t\t'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f6fc11e-c887-4e72-aa6f-aaf84b96a1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop_v2(device, epoch, test_loader, model):\n",
    "\tmodel.eval()\n",
    "\t# Iterate over batches\n",
    "\tfor batch_idx, (X, y) in enumerate(test_loader):\n",
    "\t\t# Batch start time\n",
    "\t\tstart_time = time.time()\n",
    "\t\t# Use sequence indices to slice corresponding images\n",
    "\t\t#x_seq = all_imgs[seq_ind,:,:]\n",
    "\t\tx_seq = X.float()       \n",
    "\t\t# Load data to device\n",
    "\t\tx_seq = x_seq.to(device)\n",
    "\t\ty = y.to(device)\n",
    "\t\t# Zero out gradients for optimizer \n",
    "\t\toptimizer.zero_grad()\n",
    "\t\t# Run model \n",
    "\t\t'''\n",
    "\t\tif 'MNM' in args.model_name:\n",
    "\t\t\ty_pred_linear, y_pred, const_loss = model(x_seq, device)\n",
    "\t\telse:\n",
    "\t\t\ty_pred_linear, y_pred = model(x_seq, device)\n",
    "\t\t'''\n",
    "\t\ty_pred_linear, y_pred = model(x_seq, device)\n",
    "\t\t# Loss\n",
    "\t\tloss_fn = nn.CrossEntropyLoss()\n",
    "\t\tloss = loss_fn(y_pred_linear, y)\n",
    "\t\t'''\n",
    "\t\tif 'MNM' in args.model_name:\n",
    "\t\t\tloss += const_loss\n",
    "\t\t'''\n",
    "\t\t# Update model\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\t# Batch duration\n",
    "\t\tend_time = time.time()\n",
    "\t\tbatch_dur = end_time - start_time\n",
    "\t\t# Report prgoress\n",
    "\t\t#if batch_idx % args.log_interval == 0:\n",
    "\t\tif batch_idx % 10 == 0:\n",
    "\t\t\t# Accuracy\n",
    "\t\t\tacc = torch.eq(y_pred, y).float().mean().item() * 100.0\n",
    "\t\t\t# Report \t\n",
    "\t\t\tlog.info('[Epoch: ' + str(epoch) + '] ' + \\\n",
    "\t\t\t\t\t '[Batch: ' + str(batch_idx) + ' of ' + str(len(train_loader)) + '] ' + \\\n",
    "\t\t\t\t\t '[Loss = ' + '{:.4f}'.format(loss.item()) + '] ' + \\\n",
    "\t\t\t\t\t '[Accuracy = ' + '{:.2f}'.format(acc) + '] ' + \\\n",
    "\t\t\t\t\t '[' + '{:.3f}'.format(batch_dur) + ' sec/batch]')\n",
    "\t\t\t# Save progress to file\n",
    "\t\t\t'''\n",
    "\t\t\ttrain_prog_f.write(str(batch_idx) + ' ' +\\\n",
    "\t\t\t\t\t\t\t   '{:.4f}'.format(loss.item()) + ' ' + \\\n",
    "\t\t\t\t\t\t\t   '{:.2f}'.format(acc) + '\\n')\n",
    "\t\t\t'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8f79142-cf26-4f4c-a99e-ad2fff8817fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(device, epoch, test_loader, model):\n",
    "\tlog.info('Evaluating on test set...')\n",
    "\t# Set to eval mode\n",
    "\tmodel.eval()\n",
    "\t# Iterate over batches\n",
    "\tall_acc = []\n",
    "\tall_loss = []\n",
    "\tfor batch_idx, (X, y) in enumerate(test_loader):\n",
    "\t\t# Use sequence indices to slice corresponding images\n",
    "\t\tx_seq = X.float()\n",
    "\t\t# Load data to device\n",
    "\t\tx_seq = x_seq.to(device)\n",
    "\t\ty = y.to(device)\n",
    "\t\t# Run model \n",
    "\t\t'''\n",
    "\t\tif 'MNM' in args.model_name:\n",
    "\t\t\ty_pred_linear, y_pred, const_loss = model(x_seq, device)\n",
    "\t\telse:\n",
    "\t\t\ty_pred_linear, y_pred = model(x_seq, device)\n",
    "\t\t'''\n",
    "\t\ty_pred_linear, y_pred = model(x_seq, device)\n",
    "\t\t# Loss\n",
    "\t\tloss_fn = nn.CrossEntropyLoss()\n",
    "\t\tloss = loss_fn(y_pred_linear, y)\n",
    "\t\t'''\n",
    "\t\tif 'MNM' in args.model_name:\n",
    "\t\t\tloss += const_loss\n",
    "\t\t'''\n",
    "\t\tall_loss.append(loss.item())\n",
    "\t\t# Accuracy\n",
    "\t\tacc = torch.eq(y_pred, y).float().mean().item() * 100.0\n",
    "\t\tall_acc.append(acc)\n",
    "\t\t# Report progress\n",
    "\t\tlog.info('[Batch: ' + str(batch_idx) + ' of ' + str(len(test_loader)) + ']')\n",
    "\t# Report overall test performance\n",
    "\tavg_loss = np.mean(all_loss)\n",
    "\tavg_acc = np.mean(all_acc)\n",
    "\tlog.info('[Summary] ' + \\\n",
    "\t\t\t '[Loss = ' + '{:.4f}'.format(avg_loss) + '] ' + \\\n",
    "\t\t\t '[Accuracy = ' + '{:.2f}'.format(avg_acc) + ']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "exempt-input",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[37m\u001b[01m[2021-05-18 17:02:28,453] Building encoder...\u001b[0m\n",
      "\u001b[37m\u001b[01m[2021-05-18 17:02:28,454] Building convolutional encoder...\u001b[0m\n",
      "\u001b[37m\u001b[01m[2021-05-18 17:02:28,454] Conv layers...\u001b[0m\n",
      "\u001b[37m\u001b[01m[2021-05-18 17:02:28,460] FC layers...\u001b[0m\n",
      "\u001b[37m\u001b[01m[2021-05-18 17:02:28,619] Building LSTM and output layers...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "y = tensor([5, 0, 4, 0, 6, 1, 1, 1, 6, 7, 3, 6, 5, 0, 3, 3, 2, 2, 0, 0, 3, 4, 6, 1,\n",
      "        0, 1, 2, 6, 2, 0, 2, 3])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 126.00 MiB (GPU 0; 23.70 GiB total capacity; 9.41 GiB already allocated; 120.19 MiB free; 9.47 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-896336266e45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {t+1}\\n-------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mtest_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-a3e7b6e28e77>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(device, epoch, train_loader, model, optimizer)\u001b[0m\n\u001b[1;32m     22\u001b[0m                         \u001b[0my_pred_linear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \t\t'''\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0my_pred_linear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m                 \u001b[0;31m# Loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-ae38c95a2840>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_seq, device)\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                         \u001b[0mx_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                         \u001b[0mz_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m                         \u001b[0mz_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0mz_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-29800a0312a7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0;31m# Convolutional layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                 \u001b[0mconv1_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                 \u001b[0mconv2_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv1_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mconv3_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv2_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    393\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 395\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    396\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 126.00 MiB (GPU 0; 23.70 GiB total capacity; 9.41 GiB already allocated; 120.19 MiB free; 9.47 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "learning_rate = 5e-4\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "device = torch.device(\"cuda:\" + str(0))\n",
    "model = Model().to(device)\n",
    "# Initialize the loss function\n",
    "#loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(device, t+1, train_dataloader, model, optimizer)\n",
    "test_loop(device,t, test_dataloader, model)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80d9103-7320-4d14-83d5-8e6e239bc958",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (λ)",
   "language": "python",
   "name": "lambda-stack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
