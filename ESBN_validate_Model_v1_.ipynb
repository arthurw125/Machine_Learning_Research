{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "written-mattress",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "from dist3 import create_task\n",
    "#from esbn_pytorch import ESBN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36e8bfd5-9f55-4891-9eb6-b4bdbb342cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import log\n",
    "from modules import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64341df8-307a-4b22-a0cb-ec9f08153381",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_conv(nn.Module):\n",
    "\t#def __init__(self, args):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(Encoder_conv, self).__init__()\n",
    "\t\tlog.info('Building convolutional encoder...')\n",
    "\t\t# Convolutional layers\n",
    "\t\tlog.info('Conv layers...')\n",
    "\t\t'''\n",
    "\t\tself.conv1 = nn.Conv2d(1, 32, 4, stride=2, padding=1)\n",
    "\t\tself.conv2 = nn.Conv2d(32, 32, 4, stride=2, padding=1)\n",
    "\t\tself.conv3 = nn.Conv2d(32, 32, 4, stride=2, padding=1)\n",
    "\t\t'''\n",
    "\t\tself.conv1 = nn.Conv2d(1, 160, 4, stride=2, padding=1)\n",
    "\t\tself.conv2 = nn.Conv2d(160, 160, 4, stride=2, padding=1)\n",
    "\t\tself.conv3 = nn.Conv2d(160, 160, 4, stride=2, padding=1)\n",
    "\t\t# Fully-connected layers\n",
    "\t\tlog.info('FC layers...')\n",
    "\t\t#self.fc1 = nn.Linear(4*4*32, 256)\n",
    "\t\t#self.fc2 = nn.Linear(256, 128)\n",
    "\t\tself.fc1 = nn.Linear(64000, 256)\n",
    "\t\tself.fc2 = nn.Linear(256, 128)\n",
    "\t\t# Nonlinearities\n",
    "\t\tself.relu = nn.ReLU()\n",
    "\t\t# Initialize parameters\n",
    "\t\tfor name, param in self.named_parameters():\n",
    "\t\t\t# Initialize all biases to 0\n",
    "\t\t\tif 'bias' in name:\n",
    "\t\t\t\tnn.init.constant_(param, 0.0)\n",
    "\t\t\t# Initialize all pre-ReLU weights using Kaiming normal distribution\n",
    "\t\t\telif 'weight' in name:\n",
    "\t\t\t\tnn.init.kaiming_normal_(param, nonlinearity='relu')\n",
    "\tdef forward(self, x):\n",
    "\t\t# Convolutional layers\n",
    "\t\tconv1_out = self.relu(self.conv1(x))\n",
    "\t\tconv2_out = self.relu(self.conv2(conv1_out))\n",
    "\t\tconv3_out = self.relu(self.conv3(conv2_out))\n",
    "\t\t# Flatten output of conv. net\n",
    "\t\tconv3_out_flat = torch.flatten(conv3_out, 1)\n",
    "\t\t# Fully-connected layers\n",
    "\t\t#print(\"conv3_out_flat.size() =\",conv3_out_flat.size())\n",
    "\t\t#time.sleep(120)\n",
    "\t\tfc1_out = self.relu(self.fc1(conv3_out_flat))\n",
    "\t\tfc2_out = self.relu(self.fc2(fc1_out))\n",
    "\t\t# Output\n",
    "\t\tz = fc2_out\n",
    "\t\treturn z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "brutal-rebel",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\t#def __init__(self, task_gen, args):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(Model, self).__init__()\n",
    "\t\t# Encoder\n",
    "\t\tlog.info('Building encoder...')\n",
    "\t\t'''\n",
    "\t\tif args.encoder == 'conv':\n",
    "\t\t\tself.encoder = Encoder_conv(args)\n",
    "\t\telif args.encoder == 'mlp':\n",
    "\t\t\tself.encoder = Encoder_mlp(args)\n",
    "\t\telif args.encoder == 'rand':\n",
    "\t\t\tself.encoder = Encoder_rand(args)\n",
    "\t\t'''\n",
    "\t\tself.encoder = Encoder_conv()# removed \"args\" argument\n",
    "\t\t# LSTM and output layers\n",
    "\t\tlog.info('Building LSTM and output layers...')\n",
    "\t\tself.z_size = 128\n",
    "\t\tself.key_size = 256\n",
    "\t\tself.hidden_size = 512\n",
    "\t\tself.lstm = nn.LSTM(self.key_size + 1, self.hidden_size, batch_first=True)\n",
    "\t\tself.key_w_out = nn.Linear(self.hidden_size, self.key_size)\n",
    "\t\tself.g_out = nn.Linear(self.hidden_size, 1)\n",
    "\t\tself.confidence_gain = nn.Parameter(torch.ones(1))\n",
    "\t\tself.confidence_bias = nn.Parameter(torch.zeros(1))\n",
    "\t\t#self.y_out = nn.Linear(self.hidden_size, task_gen.y_dim)\n",
    "\t\ty_out = 8 # number of outputs/ ESBN = 4\n",
    "\t\tself.y_out = nn.Linear(self.hidden_size, y_out)\n",
    "\t\t# Context normalization\n",
    "\t\t#if args.norm_type == 'contextnorm' or args.norm_type == 'tasksegmented_contextnorm':\n",
    "\t\tif True: # assumes \"contextnorm or tasksegmented_contextnorm\"\n",
    "\t\t\tself.contextnorm = True\n",
    "\t\t\tself.gamma = nn.Parameter(torch.ones(self.z_size))\n",
    "\t\t\tself.beta = nn.Parameter(torch.zeros(self.z_size))\n",
    "\t\telse:\n",
    "\t\t\tself.contextnorm = False\n",
    "\t\t'''\n",
    "\t\tif args.norm_type == 'tasksegmented_contextnorm':\n",
    "\t\t\tself.task_seg = task_gen.task_seg\n",
    "\t\telse:\n",
    "\t\t\tself.task_seg = [np.arange(task_gen.seq_len)]\n",
    "\t\t'''\n",
    "\t\tseq_len = 16 # number of images per Raven problem / ESBN = 9        \n",
    "\t\tself.task_seg = [np.arange(seq_len)]\n",
    "\t\t# Nonlinearities\n",
    "\t\tself.relu = nn.ReLU()\n",
    "\t\tself.sigmoid = nn.Sigmoid()\n",
    "\t\tself.softmax = nn.Softmax(dim=1)\n",
    "\t\t# Initialize parameters\n",
    "\t\tfor name, param in self.named_parameters():\n",
    "\t\t\t# Encoder parameters have already been initialized\n",
    "\t\t\tif not ('encoder' in name) and not ('confidence' in name):\n",
    "\t\t\t\t# Initialize all biases to 0\n",
    "\t\t\t\tif 'bias' in name:\n",
    "\t\t\t\t\tnn.init.constant_(param, 0.0)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tif 'lstm' in name:\n",
    "\t\t\t\t\t\t# Initialize gate weights (followed by sigmoid) using Xavier normal distribution\n",
    "\t\t\t\t\t\tnn.init.xavier_normal_(param[:self.hidden_size*2,:])\n",
    "\t\t\t\t\t\tnn.init.xavier_normal_(param[self.hidden_size*3:,:])\n",
    "\t\t\t\t\t\t# Initialize input->hidden and hidden->hidden weights (followed by tanh) using Xavier normal distribution with gain = \n",
    "\t\t\t\t\t\tnn.init.xavier_normal_(param[self.hidden_size*2:self.hidden_size*3,:], gain=5.0/3.0)\n",
    "\t\t\t\t\telif 'key_w' in name:\n",
    "\t\t\t\t\t\t# Initialize weights for key output layer (followed by ReLU) using Kaiming normal distribution\n",
    "\t\t\t\t\t\tnn.init.kaiming_normal_(param, nonlinearity='relu')\n",
    "\t\t\t\t\telif 'g_out' in name:\n",
    "\t\t\t\t\t\t# Initialize weights for gate output layer (followed by sigmoid) using Xavier normal distribution\n",
    "\t\t\t\t\t\tnn.init.xavier_normal_(param)\n",
    "\t\t\t\t\telif 'y_out' in name:\n",
    "\t\t\t\t\t\t# Initialize weights for multiple-choice output layer (followed by softmax) using Xavier normal distribution\n",
    "\t\t\t\t\t\tnn.init.xavier_normal_(param)\n",
    "\tdef forward(self, x_seq, device):\n",
    "\t\t# Encode all images in sequence\n",
    "\t\tz_seq = []\n",
    "\t\tfor t in range(x_seq.shape[1]):\n",
    "\t\t\tx_t = x_seq[:,t,:,:].unsqueeze(1)\n",
    "\t\t\tz_t = self.encoder(x_t)\n",
    "\t\t\tz_seq.append(z_t)\n",
    "\t\tz_seq = torch.stack(z_seq, dim=1)\n",
    "\t\tif self.contextnorm:\n",
    "\t\t\tz_seq_all_seg = []\n",
    "\t\t\tfor seg in range(len(self.task_seg)):\n",
    "\t\t\t\tz_seq_all_seg.append(self.apply_context_norm(z_seq[:,self.task_seg[seg],:]))\n",
    "\t\t\tz_seq = torch.cat(z_seq_all_seg, dim=1)\n",
    "\t\t# Initialize hidden state\n",
    "\t\thidden = torch.zeros(1, x_seq.shape[0], self.hidden_size).to(device)\n",
    "\t\tcell_state = torch.zeros(1, x_seq.shape[0], self.hidden_size).to(device)\n",
    "\t\t# Initialize retrieved key vector\n",
    "\t\tkey_r = torch.zeros(x_seq.shape[0], 1, self.key_size + 1).to(device)\n",
    "\t\t# Memory model (extra time step to process key retrieved on final time step)\n",
    "\t\tfor t in range(x_seq.shape[1] + 1):\n",
    "\t\t\t# Image embedding\n",
    "\t\t\tif t == x_seq.shape[1]:\n",
    "\t\t\t\tz_t = torch.zeros(x_seq.shape[0], 1, self.z_size).to(device)\n",
    "\t\t\telse:\n",
    "\t\t\t\tz_t = z_seq[:,t,:].unsqueeze(1)\n",
    "\t\t\t# Controller\n",
    "\t\t\t# LSTM\n",
    "\t\t\tlstm_out, (hidden, cell_state) = self.lstm(key_r, (hidden, cell_state))\n",
    "\t\t\t# Key output layers\n",
    "\t\t\tkey_w = self.relu(self.key_w_out(lstm_out))\n",
    "\t\t\t# Gates\n",
    "\t\t\tg = self.sigmoid(self.g_out(lstm_out))\n",
    "\t\t\t# Task output layer\n",
    "\t\t\ty_pred_linear = self.y_out(lstm_out).squeeze()\n",
    "\t\t\ty_pred = y_pred_linear.argmax(1)\n",
    "\t\t\t# Read from memory\n",
    "\t\t\tif t == 0:\n",
    "\t\t\t\tkey_r = torch.zeros(x_seq.shape[0], 1, self.key_size + 1).to(device)\n",
    "\t\t\telse:\n",
    "\t\t\t\t# Read key\n",
    "\t\t\t\tw_k = self.softmax((z_t * M_v).sum(dim=2))\n",
    "\t\t\t\tc_k = self.sigmoid(((z_t * M_v).sum(dim=2) * self.confidence_gain) + self.confidence_bias)\n",
    "\t\t\t\tkey_r = g * (torch.cat([M_k, c_k.unsqueeze(2)], dim=2) * w_k.unsqueeze(2)).sum(1).unsqueeze(1)\n",
    "\t\t\t# Write to memory\n",
    "\t\t\tif t == 0:\n",
    "\t\t\t\tM_k = key_w\n",
    "\t\t\t\tM_v = z_t\n",
    "\t\t\telse:\n",
    "\t\t\t\tM_k = torch.cat([M_k, key_w], dim=1)\n",
    "\t\t\t\tM_v = torch.cat([M_v, z_t], dim=1)\n",
    "\t\treturn y_pred_linear, y_pred\n",
    "\tdef apply_context_norm(self, z_seq):\n",
    "\t\teps = 1e-8\n",
    "\t\tz_mu = z_seq.mean(1)\n",
    "\t\tz_sigma = (z_seq.var(1) + eps).sqrt()\n",
    "\t\tz_seq = (z_seq - z_mu.unsqueeze(1)) / z_sigma.unsqueeze(1)\n",
    "\t\tz_seq = (z_seq * self.gamma) + self.beta\n",
    "\t\treturn z_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "piano-inspection",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_npz_img(img_path):\n",
    "    data = np.load(img_path)\n",
    "    img = data['image']\n",
    "    target = data['target']\n",
    "    x = img[:,:,:]\n",
    "    #x = np.expand_dims(x, axis=0)\n",
    "    #x = x.reshape((x.shape[0],x.shape[1],x.shape[2],x.shape[3],1))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "standing-arthur",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_npz_target(target_path):\n",
    "    data = np.load(target_path)\n",
    "    target = data['target']\n",
    "    y = int(target)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "motivated-dream",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(folder,num_imgs,config):\n",
    "    X = []\n",
    "    #X = np.array(X)\n",
    "    Y =[]\n",
    "    #for i in range(num_imgs):\n",
    "    name = ''\n",
    "    if config=='train':\n",
    "        name = 'train'\n",
    "    elif config=='validate':\n",
    "        name = 'val'\n",
    "    else:\n",
    "        name = 'test'\n",
    "        \n",
    "    count = 0\n",
    "    i = 0\n",
    "    while count < num_imgs:\n",
    "        try:\n",
    "            x = grab_npz_img('/home/asw3x/RAVEN-10000/'+folder+'/RAVEN_%d_%s.npz'%(i,name))\n",
    "            y = grab_npz_target('/home/asw3x/RAVEN-10000/'+folder+'/RAVEN_%d_%s.npz'%(i,name))\n",
    "            i += 1\n",
    "        except:\n",
    "            i += 1\n",
    "            continue\n",
    "        X.append(x)\n",
    "        #X = np.concatenate(x)\n",
    "        Y.append(y)\n",
    "        count += 1\n",
    "    X = np.array(X)\n",
    "    X = np.squeeze(X)\n",
    "    #X = np.expand_dims(X, axis=4)\n",
    "    #X = X.reshape((X.shape[0],X.shape[2],X.shape[3],X.shape[1]))\n",
    "    #X = np.moveaxis(X, 1, -1)\n",
    "    return X,np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "stainless-impact",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_task(size=1000, task=\"center_single\"):\n",
    "    folder = task\n",
    "    train_size = size\n",
    "    test_size = train_size*0.4\n",
    "    \n",
    "    X_train, Y_train = create_dataset(folder,train_size,\"train\")\n",
    "    X_test, Y_test = create_dataset(folder,test_size,\"test\")\n",
    "    \n",
    "    # Create training and test sets\n",
    "    train_set = {'img_seq': X_train, 'y': Y_train}\n",
    "    test_set = {'img_seq': X_test, 'y': Y_test}\n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "surrounded-cooperation",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set,test_set = create_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "entertaining-acceptance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 16, 160, 160)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set['img_seq'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "photographic-actress",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set['y'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "structural-response",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None, target_transform=None):\n",
    "        '''\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        '''\n",
    "        self.img_seq = dataset['img_seq']\n",
    "        self.y = dataset['y']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        sample = {\"image\": image, \"label\": label}\n",
    "        '''\n",
    "        img_seq = self.img_seq[idx,:,:,:]\n",
    "        y = self.y[idx]\n",
    "        \n",
    "        return img_seq, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "moving-purpose",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = CustomImageDataset(training_set)\n",
    "train_dataloader = DataLoader(training_set, batch_size=32, shuffle=True)\n",
    "test_set = CustomImageDataset(test_set)\n",
    "test_dataloader = DataLoader(test_set, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "obvious-robinson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([32, 16, 160, 160])\n",
      "Labels batch shape: torch.Size([32])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 0\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0,0,:,:].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "excessive-loading",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([32, 16, 160, 160])\n",
      "Labels batch shape: torch.Size([32])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 1\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0,0,:,:].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "romantic-feelings",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef train_loop(train_dataloader, model, loss_fn, optimizer):\\n    size = len(dataloader.dataset)\\n    for batch, (X, y) in enumerate(dataloader):\\n        # Compute prediction and loss\\n        pred = model(X)\\n        loss = loss_fn(pred, y)\\n\\n        # Backpropagation\\n        optimizer.zero_grad()\\n        loss.backward()\\n        optimizer.step()\\n\\n        if batch % 128 == 0:\\n            loss, current = loss.item(), batch * len(X)\\n            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def train_loop(train_dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 128 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c779a5f-ebda-4176-bfe5-8ec303bd230a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(device, epoch, train_loader, model, optimizer):\n",
    "\tmodel.train()\n",
    "\t# Iterate over batches\n",
    "\tfor batch_idx, (X, y) in enumerate(train_loader):\n",
    "\t\t# Batch start time\n",
    "\t\tstart_time = time.time()\n",
    "\t\t# Use sequence indices to slice corresponding images\n",
    "\t\t#x_seq = all_imgs[seq_ind,:,:]\n",
    "\t\tx_seq = X.float()       \n",
    "\t\t# Load data to device\n",
    "\t\tx_seq = x_seq.to(device)\n",
    "\t\ty = y.to(device)\n",
    "\t\t# Zero out gradients for optimizer \n",
    "\t\toptimizer.zero_grad()\n",
    "\t\t# Run model \n",
    "\t\t'''\n",
    "\t\tif 'MNM' in args.model_name:\n",
    "\t\t\ty_pred_linear, y_pred, const_loss = model(x_seq, device)\n",
    "\t\telse:\n",
    "\t\t\ty_pred_linear, y_pred = model(x_seq, device)\n",
    "\t\t'''\n",
    "\t\ty_pred_linear, y_pred = model(x_seq, device)\n",
    "\t\t# Loss\n",
    "\t\tloss_fn = nn.CrossEntropyLoss()\n",
    "\t\tloss = loss_fn(y_pred_linear, y)\n",
    "\t\t'''\n",
    "\t\tif 'MNM' in args.model_name:\n",
    "\t\t\tloss += const_loss\n",
    "\t\t'''\n",
    "\t\t# Update model\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\t# Batch duration\n",
    "\t\tend_time = time.time()\n",
    "\t\tbatch_dur = end_time - start_time\n",
    "\t\t# Report prgoress\n",
    "\t\t#if batch_idx % args.log_interval == 0:\n",
    "\t\tif batch_idx % 50 == 0:\n",
    "\t\t\t# Accuracy\n",
    "\t\t\tacc = torch.eq(y_pred, y).float().mean().item() * 100.0\n",
    "\t\t\t# Report \t\n",
    "\t\t\tlog.info('[Epoch: ' + str(epoch) + '] ' + \\\n",
    "\t\t\t\t\t '[Batch: ' + str(batch_idx) + ' of ' + str(len(train_loader)) + '] ' + \\\n",
    "\t\t\t\t\t '[Loss = ' + '{:.4f}'.format(loss.item()) + '] ' + \\\n",
    "\t\t\t\t\t '[Accuracy = ' + '{:.2f}'.format(acc) + '] ' + \\\n",
    "\t\t\t\t\t '[' + '{:.3f}'.format(batch_dur) + ' sec/batch]')\n",
    "\t\t\t# Save progress to file\n",
    "\t\t\t'''\n",
    "\t\t\ttrain_prog_f.write(str(batch_idx) + ' ' +\\\n",
    "\t\t\t\t\t\t\t   '{:.4f}'.format(loss.item()) + ' ' + \\\n",
    "\t\t\t\t\t\t\t   '{:.2f}'.format(acc) + '\\n')\n",
    "\t\t\t'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f6fc11e-c887-4e72-aa6f-aaf84b96a1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop_v2(device, epoch, test_loader, model):\n",
    "\tmodel.eval()\n",
    "\t# Iterate over batches\n",
    "\tfor batch_idx, (X, y) in enumerate(test_loader):\n",
    "\t\t# Batch start time\n",
    "\t\tstart_time = time.time()\n",
    "\t\t# Use sequence indices to slice corresponding images\n",
    "\t\t#x_seq = all_imgs[seq_ind,:,:]\n",
    "\t\tx_seq = X.float()       \n",
    "\t\t# Load data to device\n",
    "\t\tx_seq = x_seq.to(device)\n",
    "\t\ty = y.to(device)\n",
    "\t\t# Zero out gradients for optimizer \n",
    "\t\toptimizer.zero_grad()\n",
    "\t\t# Run model \n",
    "\t\t'''\n",
    "\t\tif 'MNM' in args.model_name:\n",
    "\t\t\ty_pred_linear, y_pred, const_loss = model(x_seq, device)\n",
    "\t\telse:\n",
    "\t\t\ty_pred_linear, y_pred = model(x_seq, device)\n",
    "\t\t'''\n",
    "\t\ty_pred_linear, y_pred = model(x_seq, device)\n",
    "\t\t# Loss\n",
    "\t\tloss_fn = nn.CrossEntropyLoss()\n",
    "\t\tloss = loss_fn(y_pred_linear, y)\n",
    "\t\t'''\n",
    "\t\tif 'MNM' in args.model_name:\n",
    "\t\t\tloss += const_loss\n",
    "\t\t'''\n",
    "\t\t# Update model\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\t# Batch duration\n",
    "\t\tend_time = time.time()\n",
    "\t\tbatch_dur = end_time - start_time\n",
    "\t\t# Report prgoress\n",
    "\t\t#if batch_idx % args.log_interval == 0:\n",
    "\t\tif batch_idx % 10 == 0:\n",
    "\t\t\t# Accuracy\n",
    "\t\t\tacc = torch.eq(y_pred, y).float().mean().item() * 100.0\n",
    "\t\t\t# Report \t\n",
    "\t\t\tlog.info('[Epoch: ' + str(epoch) + '] ' + \\\n",
    "\t\t\t\t\t '[Batch: ' + str(batch_idx) + ' of ' + str(len(train_loader)) + '] ' + \\\n",
    "\t\t\t\t\t '[Loss = ' + '{:.4f}'.format(loss.item()) + '] ' + \\\n",
    "\t\t\t\t\t '[Accuracy = ' + '{:.2f}'.format(acc) + '] ' + \\\n",
    "\t\t\t\t\t '[' + '{:.3f}'.format(batch_dur) + ' sec/batch]')\n",
    "\t\t\t# Save progress to file\n",
    "\t\t\t'''\n",
    "\t\t\ttrain_prog_f.write(str(batch_idx) + ' ' +\\\n",
    "\t\t\t\t\t\t\t   '{:.4f}'.format(loss.item()) + ' ' + \\\n",
    "\t\t\t\t\t\t\t   '{:.2f}'.format(acc) + '\\n')\n",
    "\t\t\t'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8f79142-cf26-4f4c-a99e-ad2fff8817fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(device, epoch, test_loader, model):\n",
    "\tlog.info('Evaluating on test set...')\n",
    "\t# Set to eval mode\n",
    "\tmodel.eval()\n",
    "\t# Iterate over batches\n",
    "\tall_acc = []\n",
    "\tall_loss = []\n",
    "\tfor batch_idx, (X, y) in enumerate(test_loader):\n",
    "\t\t# Use sequence indices to slice corresponding images\n",
    "\t\tx_seq = X.float()\n",
    "\t\t# Load data to device\n",
    "\t\tx_seq = x_seq.to(device)\n",
    "\t\ty = y.to(device)\n",
    "\t\t# Run model \n",
    "\t\t'''\n",
    "\t\tif 'MNM' in args.model_name:\n",
    "\t\t\ty_pred_linear, y_pred, const_loss = model(x_seq, device)\n",
    "\t\telse:\n",
    "\t\t\ty_pred_linear, y_pred = model(x_seq, device)\n",
    "\t\t'''\n",
    "\t\ty_pred_linear, y_pred = model(x_seq, device)\n",
    "\t\t# Loss\n",
    "\t\tloss_fn = nn.CrossEntropyLoss()\n",
    "\t\tloss = loss_fn(y_pred_linear, y)\n",
    "\t\t'''\n",
    "\t\tif 'MNM' in args.model_name:\n",
    "\t\t\tloss += const_loss\n",
    "\t\t'''\n",
    "\t\tall_loss.append(loss.item())\n",
    "\t\t# Accuracy\n",
    "\t\tacc = torch.eq(y_pred, y).float().mean().item() * 100.0\n",
    "\t\tall_acc.append(acc)\n",
    "\t\t# Report progress\n",
    "\t\tlog.info('[Batch: ' + str(batch_idx) + ' of ' + str(len(test_loader)) + ']')\n",
    "\t# Report overall test performance\n",
    "\tavg_loss = np.mean(all_loss)\n",
    "\tavg_acc = np.mean(all_acc)\n",
    "\tlog.info('[Summary] ' + \\\n",
    "\t\t\t '[Loss = ' + '{:.4f}'.format(avg_loss) + '] ' + \\\n",
    "\t\t\t '[Accuracy = ' + '{:.2f}'.format(avg_acc) + ']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exempt-input",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[37m\u001b[01m[2021-05-18 16:11:06,927] Building encoder...\u001b[0m\n",
      "\u001b[37m\u001b[01m[2021-05-18 16:11:06,928] Building convolutional encoder...\u001b[0m\n",
      "\u001b[37m\u001b[01m[2021-05-18 16:11:06,929] Conv layers...\u001b[0m\n",
      "\u001b[37m\u001b[01m[2021-05-18 16:11:06,935] FC layers...\u001b[0m\n",
      "\u001b[37m\u001b[01m[2021-05-18 16:11:07,086] Building LSTM and output layers...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[37m\u001b[01m[2021-05-18 16:11:09,638] [Epoch: 1] [Batch: 0 of 32] [Loss = 2.0668] [Accuracy = 15.62] [0.164 sec/batch]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[37m\u001b[01m[2021-05-18 16:11:19,746] [Epoch: 2] [Batch: 0 of 32] [Loss = 2.0836] [Accuracy = 6.25] [0.205 sec/batch]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[37m\u001b[01m[2021-05-18 16:11:29,921] [Epoch: 3] [Batch: 0 of 32] [Loss = 2.0849] [Accuracy = 12.50] [0.206 sec/batch]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[37m\u001b[01m[2021-05-18 16:11:40,124] [Epoch: 4] [Batch: 0 of 32] [Loss = 2.0790] [Accuracy = 6.25] [0.206 sec/batch]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[37m\u001b[01m[2021-05-18 16:11:50,336] [Epoch: 5] [Batch: 0 of 32] [Loss = 2.0847] [Accuracy = 6.25] [0.208 sec/batch]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 5e-4\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "device = torch.device(\"cuda:\" + str(0))\n",
    "model = Model().to(device)\n",
    "# Initialize the loss function\n",
    "#loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(device, t+1, train_dataloader, model, optimizer)\n",
    "test_loop(device,t, test_dataloader, model)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80d9103-7320-4d14-83d5-8e6e239bc958",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (λ)",
   "language": "python",
   "name": "lambda-stack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
