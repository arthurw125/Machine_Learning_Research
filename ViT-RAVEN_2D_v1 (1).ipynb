{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "mineral-corps",
   "metadata": {},
   "source": [
    "# ViT Example on MNIST\n",
    "\n",
    "Adapted from: [https://keras.io/examples/vision/image_classification_with_vision_transformer/](https://keras.io/examples/vision/image_classification_with_vision_transformer/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "narrow-redhead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_addons in /opt/conda/lib/python3.9/site-packages (0.15.0)\n",
      "Requirement already satisfied: typeguard>=2.7 in /opt/conda/lib/python3.9/site-packages (from tensorflow_addons) (2.13.0)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "!pip install tensorflow_addons\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "olympic-indiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_npz_img(img_path):\n",
    "    data = np.load(img_path)\n",
    "    img = data['image']\n",
    "    target = data['target']\n",
    "    x = img[:,:,:]\n",
    "    #x = np.expand_dims(x, axis=0)\n",
    "    #x = x.reshape((x.shape[0],x.shape[1],x.shape[2],x.shape[3],1))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "unavailable-tragedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_npz_target(target_path):\n",
    "    data = np.load(target_path)\n",
    "    target = data['target']\n",
    "    y = int(target)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "third-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(folder,num_imgs,config):\n",
    "    X = []\n",
    "    #X = np.array(X)\n",
    "    Y =[]\n",
    "    #for i in range(num_imgs):\n",
    "    name = ''\n",
    "    if config=='train':\n",
    "        name = 'train'\n",
    "    elif config=='validate':\n",
    "        name = 'val'\n",
    "    else:\n",
    "        name = 'test'\n",
    "        \n",
    "    count = 0\n",
    "    i = 0\n",
    "    while count < num_imgs:\n",
    "        try:\n",
    "            x = grab_npz_img('/home/asw3x/RAVEN-10000/'+folder+'/RAVEN_%d_%s.npz'%(i,name))\n",
    "            y = grab_npz_target('/home/asw3x/RAVEN-10000/'+folder+'/RAVEN_%d_%s.npz'%(i,name))\n",
    "            i += 1\n",
    "        except:\n",
    "            i += 1\n",
    "            continue\n",
    "        X.append(x)\n",
    "        #X = np.concatenate(x)\n",
    "        Y.append(y)\n",
    "        print(count, end='\\r', flush=True)\n",
    "        count += 1\n",
    "    X = np.array(X)\n",
    "    X = np.squeeze(X)\n",
    "    #X = np.expand_dims(X, axis=4)\n",
    "    #X = X.reshape((X.shape[0],X.shape[2],X.shape[3],X.shape[1]))\n",
    "    X = np.moveaxis(X, 1, -1)\n",
    "    return X,np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "worthy-fishing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_3d(folder,num_imgs,config):\n",
    "    X = []\n",
    "    #X = np.array(X)\n",
    "    Y =[]\n",
    "    #for i in range(num_imgs):\n",
    "    \n",
    "    name = ''\n",
    "    if config=='train':\n",
    "        name = 'train'\n",
    "    elif config=='validate':\n",
    "        name = 'val'\n",
    "    else:\n",
    "        name = 'test'\n",
    "        \n",
    "    count = 0\n",
    "    i = 0\n",
    "    while count < num_imgs:\n",
    "        try:\n",
    "            x = grab_npz_img('/home/asw3x/RAVEN-10000/'+folder+'/RAVEN_%d_%s.npz'%(i,name))\n",
    "            y = grab_npz_target('/home/asw3x/RAVEN-10000/'+folder+'/RAVEN_%d_%s.npz'%(i,name))\n",
    "            i += 1\n",
    "        except:\n",
    "            i += 1\n",
    "            continue\n",
    "        X.append(x)\n",
    "        #X = np.concatenate(x)\n",
    "        Y.append(y)\n",
    "        print(count, end='\\r', flush=True)\n",
    "        count += 1\n",
    "    X = np.array(X)\n",
    "    X = np.squeeze(X)\n",
    "    X = np.expand_dims(X, axis=4)\n",
    "    return X,np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "closing-equilibrium",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr = 0.01, embed_dim = 8192.0, warmup_steps = 100.0):\n",
    "    arg1 = tf.math.rsqrt(tf.cast(epoch,'float32'))\n",
    "    arg2 = epoch * (warmup_steps ** -1.5)\n",
    "    return tf.math.rsqrt(embed_dim) * tf.math.minimum(arg1,arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adjacent-album",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\r"
     ]
    }
   ],
   "source": [
    "#folder = 'center_single'\n",
    "folder = 'distribute_four'\n",
    "#folder = 'in_center_single_out_center_single'\n",
    "num_imgs = 100\n",
    "val_split = 0.1\n",
    "x_train,y_train = create_dataset(folder,num_imgs,'train')\n",
    "x_val,y_val = create_dataset(folder,num_imgs*val_split,'validate')\n",
    "x_test,y_test = create_dataset(folder,num_imgs*val_split,'test')\n",
    "x_train = x_train.astype('float32')\n",
    "x_val = x_val.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "#x_train /= 255\n",
    "#x_val /= 255\n",
    "#x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "surface-welsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train ,num_classes=8)\n",
    "y_val = keras.utils.to_categorical(y_val,num_classes=8)\n",
    "y_test = keras.utils.to_categorical(y_test,num_classes=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67208fdd-ccb1-499e-90f2-43ab0317a9a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 160, 160, 8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:,:,:,-8:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a5d78de-428b-4f34-94a0-c7a2a0172f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nx_train = x_train[:,:,:,-8:]\\nx_test = x_test[:,:,:,-8:]\\nx_val = x_val[:,:,:,-8:]\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "x_train = x_train[:,:,:,-8:]\n",
    "x_test = x_test[:,:,:,-8:]\n",
    "x_val = x_val[:,:,:,-8:]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eligible-battery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 160, 160, 16)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(100, 8)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10, 160, 160, 16)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10, 8)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "'''\n",
    "display(x_train.shape)\n",
    "display(y_train.shape)\n",
    "display(x_test.shape)\n",
    "display(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "defined-speaker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nx_train = x_train.astype('float32').reshape(x_train.shape+(1,))\\nx_test = x_test.astype('float32').reshape(x_test.shape+(1,))\\nx_train /= 255\\nx_test /= 255\\n\\n# Convert class vector [0-9] to categorical assignments (one-hot)\\n# y_train = keras.utils.to_categorical(y_train, len(np.unique(y_train)))\\n# y_test = keras.utils.to_categorical(y_test, len(np.unique(y_test)))\\n\\ndisplay(x_train.shape)\\ndisplay(y_train.shape)\\ndisplay(x_test.shape)\\ndisplay(y_test.shape)\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Standardize the -input- data between 0.0-1.0 (real)\n",
    "## instead of the default 0-255 (integer)\n",
    "'''\n",
    "x_train = x_train.astype('float32').reshape(x_train.shape+(1,))\n",
    "x_test = x_test.astype('float32').reshape(x_test.shape+(1,))\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# Convert class vector [0-9] to categorical assignments (one-hot)\n",
    "# y_train = keras.utils.to_categorical(y_train, len(np.unique(y_train)))\n",
    "# y_test = keras.utils.to_categorical(y_test, len(np.unique(y_test)))\n",
    "\n",
    "display(x_train.shape)\n",
    "display(y_train.shape)\n",
    "display(x_test.shape)\n",
    "display(y_test.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "trying-centre",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 8\n",
    "#input_shape = (160, 160, 8)\n",
    "input_shape = (160, 160, 16)\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 64\n",
    "num_epochs = 1000\n",
    "image_size = 160  # We'll resize input images to this size\n",
    "patch_size = 40 # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 16\n",
    "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "minute-refund",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-15 18:18:40.204740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-15 18:18:40.205908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-15 18:18:40.223308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-15 18:18:40.224434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-15 18:18:40.225515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-15 18:18:40.226601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-15 18:18:40.228142: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-15 18:18:40.391451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-15 18:18:40.392202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-15 18:18:40.392909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-15 18:18:40.393619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-15 18:18:40.394330: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-15 18:18:40.395039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-15 18:18:40.891856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-15 18:18:40.892652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-15 18:18:40.893373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-15 18:18:40.894109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-15 18:18:40.894827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-15 18:18:40.895551: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22318 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2021-11-15 18:18:40.895910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-15 18:18:40.896619: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22320 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:21:00.0, compute capability: 8.6\n",
      "2021-11-15 18:18:41.405698: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.experimental.preprocessing.Normalization(),\n",
    "        layers.experimental.preprocessing.Resizing(image_size, image_size),\n",
    "        layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
    "        layers.experimental.preprocessing.RandomRotation(factor=0.02),\n",
    "        layers.experimental.preprocessing.RandomZoom(\n",
    "            height_factor=0.2, width_factor=0.2\n",
    "        ),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "interstate-cheat",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "suitable-mayor",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, embed_dim):\n",
    "        super(PositionEmbedding, self).__init__()\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-2] # x already embedded\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "physical-complex",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vit_classifier():\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    '''\n",
    "    inputs = layers.Input(shape=(x_train.shape[1],\n",
    "                                            x_train.shape[2],\n",
    "                                            x_train.shape[3],\n",
    "                                            x_train.shape[4]))\n",
    "    '''\n",
    "    # Augment data.\n",
    "    #augmented = data_augmentation(inputs)\n",
    "    augmented = inputs\n",
    "    # Create patches.\n",
    "    # patches = Patches(patch_size)(augmented)\n",
    "    # Encode patches.\n",
    "    # encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "    \n",
    "    patches = keras.layers.Conv2D(projection_dim,\n",
    "                                  kernel_size=(patch_size,patch_size),\n",
    "                                  strides=(patch_size,patch_size))(augmented)\n",
    "    \n",
    "    #augmented = keras.layers.Reshape((160, 160, 16, 1))(augmented)\n",
    "    '''\n",
    "    patches = keras.layers.Conv3D(projection_dim,\n",
    "                                  kernel_size=(patch_size,patch_size,1),\n",
    "                                  strides=(patch_size,patch_size,1))(augmented)\n",
    "    '''\n",
    "    patches = keras.layers.Reshape((-1,projection_dim))(patches)\n",
    "    position_embedding = PositionEmbedding(patches.shape[-2],\n",
    "                                           projection_dim)\n",
    "    encoded_patches = position_embedding(patches)\n",
    "    \n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    #representation = layers.Flatten()(representation)\n",
    "    representation = layers.GlobalAveragePooling1D()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    # Classify outputs.\n",
    "    logits = layers.Dense(num_classes)(features)\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "alternate-elder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(model):\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\n",
    "            keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "            keras.metrics.TopKCategoricalAccuracy(4, name=\"top-4-accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    checkpoint_filepath = \"/tmp/checkpoint\"\n",
    "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_filepath,\n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "    )\n",
    "    scheduler_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        validation_data=(x_val, y_val),\n",
    "        #validation_split=0.1,\n",
    "        callbacks=[checkpoint_callback,scheduler_callback],\n",
    "    )\n",
    "\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "046ef86b-9f61-4aa8-b870-f0b55ac5cab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 160, 160, 16 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 4, 4, 64)     1638464     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 16, 64)       0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "position_embedding (PositionEmb (None, 16, 64)       1024        reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization (LayerNorma (None, 16, 64)       128         position_embedding[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention (MultiHead (None, 16, 64)       66368       layer_normalization[0][0]        \n",
      "                                                                 layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 16, 64)       0           multi_head_attention[0][0]       \n",
      "                                                                 position_embedding[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 16, 64)       128         add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16, 128)      8320        layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 16, 128)      0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16, 64)       8256        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 16, 64)       0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 16, 64)       0           dropout_1[0][0]                  \n",
      "                                                                 add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_2 (LayerNor (None, 16, 64)       128         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_1 (MultiHe (None, 16, 64)       66368       layer_normalization_2[0][0]      \n",
      "                                                                 layer_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 16, 64)       0           multi_head_attention_1[0][0]     \n",
      "                                                                 add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_3 (LayerNor (None, 16, 64)       128         add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16, 128)      8320        layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 16, 128)      0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16, 64)       8256        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 16, 64)       0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 16, 64)       0           dropout_3[0][0]                  \n",
      "                                                                 add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_4 (LayerNor (None, 16, 64)       128         add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_2 (MultiHe (None, 16, 64)       66368       layer_normalization_4[0][0]      \n",
      "                                                                 layer_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 16, 64)       0           multi_head_attention_2[0][0]     \n",
      "                                                                 add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_5 (LayerNor (None, 16, 64)       128         add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16, 128)      8320        layer_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 16, 128)      0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16, 64)       8256        dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 16, 64)       0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 16, 64)       0           dropout_5[0][0]                  \n",
      "                                                                 add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_6 (LayerNor (None, 16, 64)       128         add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_3 (MultiHe (None, 16, 64)       66368       layer_normalization_6[0][0]      \n",
      "                                                                 layer_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 16, 64)       0           multi_head_attention_3[0][0]     \n",
      "                                                                 add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_7 (LayerNor (None, 16, 64)       128         add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16, 128)      8320        layer_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 16, 128)      0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16, 64)       8256        dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 16, 64)       0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 16, 64)       0           dropout_7[0][0]                  \n",
      "                                                                 add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_8 (LayerNor (None, 16, 64)       128         add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_4 (MultiHe (None, 16, 64)       66368       layer_normalization_8[0][0]      \n",
      "                                                                 layer_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 16, 64)       0           multi_head_attention_4[0][0]     \n",
      "                                                                 add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_9 (LayerNor (None, 16, 64)       128         add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16, 128)      8320        layer_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 16, 128)      0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16, 64)       8256        dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 16, 64)       0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 16, 64)       0           dropout_9[0][0]                  \n",
      "                                                                 add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_10 (LayerNo (None, 16, 64)       128         add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_5 (MultiHe (None, 16, 64)       66368       layer_normalization_10[0][0]     \n",
      "                                                                 layer_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 16, 64)       0           multi_head_attention_5[0][0]     \n",
      "                                                                 add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_11 (LayerNo (None, 16, 64)       128         add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16, 128)      8320        layer_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 16, 128)      0           dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16, 64)       8256        dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 16, 64)       0           dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 16, 64)       0           dropout_11[0][0]                 \n",
      "                                                                 add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_12 (LayerNo (None, 16, 64)       128         add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_6 (MultiHe (None, 16, 64)       66368       layer_normalization_12[0][0]     \n",
      "                                                                 layer_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 16, 64)       0           multi_head_attention_6[0][0]     \n",
      "                                                                 add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_13 (LayerNo (None, 16, 64)       128         add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 16, 128)      8320        layer_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 16, 128)      0           dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 16, 64)       8256        dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 16, 64)       0           dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 16, 64)       0           dropout_13[0][0]                 \n",
      "                                                                 add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_14 (LayerNo (None, 16, 64)       128         add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_7 (MultiHe (None, 16, 64)       66368       layer_normalization_14[0][0]     \n",
      "                                                                 layer_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 16, 64)       0           multi_head_attention_7[0][0]     \n",
      "                                                                 add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_15 (LayerNo (None, 16, 64)       128         add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 16, 128)      8320        layer_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 16, 128)      0           dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 16, 64)       8256        dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 16, 64)       0           dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 16, 64)       0           dropout_15[0][0]                 \n",
      "                                                                 add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_16 (LayerNo (None, 16, 64)       128         add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_8 (MultiHe (None, 16, 64)       66368       layer_normalization_16[0][0]     \n",
      "                                                                 layer_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 16, 64)       0           multi_head_attention_8[0][0]     \n",
      "                                                                 add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_17 (LayerNo (None, 16, 64)       128         add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 16, 128)      8320        layer_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 16, 128)      0           dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 16, 64)       8256        dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 16, 64)       0           dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 16, 64)       0           dropout_17[0][0]                 \n",
      "                                                                 add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_18 (LayerNo (None, 16, 64)       128         add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_9 (MultiHe (None, 16, 64)       66368       layer_normalization_18[0][0]     \n",
      "                                                                 layer_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 16, 64)       0           multi_head_attention_9[0][0]     \n",
      "                                                                 add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_19 (LayerNo (None, 16, 64)       128         add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 16, 128)      8320        layer_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 16, 128)      0           dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 16, 64)       8256        dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 16, 64)       0           dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 16, 64)       0           dropout_19[0][0]                 \n",
      "                                                                 add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_20 (LayerNo (None, 16, 64)       128         add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_10 (MultiH (None, 16, 64)       66368       layer_normalization_20[0][0]     \n",
      "                                                                 layer_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 16, 64)       0           multi_head_attention_10[0][0]    \n",
      "                                                                 add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_21 (LayerNo (None, 16, 64)       128         add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 16, 128)      8320        layer_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 16, 128)      0           dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 16, 64)       8256        dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 16, 64)       0           dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, 16, 64)       0           dropout_21[0][0]                 \n",
      "                                                                 add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_22 (LayerNo (None, 16, 64)       128         add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_11 (MultiH (None, 16, 64)       66368       layer_normalization_22[0][0]     \n",
      "                                                                 layer_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, 16, 64)       0           multi_head_attention_11[0][0]    \n",
      "                                                                 add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_23 (LayerNo (None, 16, 64)       128         add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 16, 128)      8320        layer_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 16, 128)      0           dense_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 16, 64)       8256        dropout_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 16, 64)       0           dense_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_23 (Add)                    (None, 16, 64)       0           dropout_23[0][0]                 \n",
      "                                                                 add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_24 (LayerNo (None, 16, 64)       128         add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_12 (MultiH (None, 16, 64)       66368       layer_normalization_24[0][0]     \n",
      "                                                                 layer_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_24 (Add)                    (None, 16, 64)       0           multi_head_attention_12[0][0]    \n",
      "                                                                 add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_25 (LayerNo (None, 16, 64)       128         add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 16, 128)      8320        layer_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 16, 128)      0           dense_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 16, 64)       8256        dropout_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)            (None, 16, 64)       0           dense_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_25 (Add)                    (None, 16, 64)       0           dropout_25[0][0]                 \n",
      "                                                                 add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_26 (LayerNo (None, 16, 64)       128         add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_13 (MultiH (None, 16, 64)       66368       layer_normalization_26[0][0]     \n",
      "                                                                 layer_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_26 (Add)                    (None, 16, 64)       0           multi_head_attention_13[0][0]    \n",
      "                                                                 add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_27 (LayerNo (None, 16, 64)       128         add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 16, 128)      8320        layer_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)            (None, 16, 128)      0           dense_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 16, 64)       8256        dropout_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, 16, 64)       0           dense_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_27 (Add)                    (None, 16, 64)       0           dropout_27[0][0]                 \n",
      "                                                                 add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_28 (LayerNo (None, 16, 64)       128         add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_14 (MultiH (None, 16, 64)       66368       layer_normalization_28[0][0]     \n",
      "                                                                 layer_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_28 (Add)                    (None, 16, 64)       0           multi_head_attention_14[0][0]    \n",
      "                                                                 add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_29 (LayerNo (None, 16, 64)       128         add_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 16, 128)      8320        layer_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)            (None, 16, 128)      0           dense_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 16, 64)       8256        dropout_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)            (None, 16, 64)       0           dense_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_29 (Add)                    (None, 16, 64)       0           dropout_29[0][0]                 \n",
      "                                                                 add_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_30 (LayerNo (None, 16, 64)       128         add_29[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_15 (MultiH (None, 16, 64)       66368       layer_normalization_30[0][0]     \n",
      "                                                                 layer_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_30 (Add)                    (None, 16, 64)       0           multi_head_attention_15[0][0]    \n",
      "                                                                 add_29[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_31 (LayerNo (None, 16, 64)       128         add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (None, 16, 128)      8320        layer_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_30 (Dropout)            (None, 16, 128)      0           dense_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_31 (Dense)                (None, 16, 64)       8256        dropout_30[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)            (None, 16, 64)       0           dense_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_31 (Add)                    (None, 16, 64)       0           dropout_31[0][0]                 \n",
      "                                                                 add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_32 (LayerNo (None, 16, 64)       128         add_31[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 64)           0           layer_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_32 (Dropout)            (None, 64)           0           global_average_pooling1d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense_32 (Dense)                (None, 2048)         133120      dropout_32[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_33 (Dropout)            (None, 2048)         0           dense_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_33 (Dense)                (None, 1024)         2098176     dropout_33[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_34 (Dropout)            (None, 1024)         0           dense_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_34 (Dense)                (None, 8)            8200        dropout_34[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 5,210,312\n",
      "Trainable params: 5,210,312\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-15 18:19:11.762016: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8201\n",
      "2021-11-15 18:19:14.161320: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 16s 1s/step - loss: 2.1159 - accuracy: 0.1300 - top-4-accuracy: 0.5100 - val_loss: 2.0881 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.4000\n",
      "Epoch 2/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 2.0946 - accuracy: 0.1100 - top-4-accuracy: 0.4600 - val_loss: 2.0863 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.6000\n",
      "Epoch 3/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 2.0791 - accuracy: 0.1900 - top-4-accuracy: 0.5900 - val_loss: 2.1036 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 4/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 2.0697 - accuracy: 0.1600 - top-4-accuracy: 0.6400 - val_loss: 2.1187 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 5/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 2.0685 - accuracy: 0.1500 - top-4-accuracy: 0.5700 - val_loss: 2.1274 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.4000\n",
      "Epoch 6/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 2.0565 - accuracy: 0.1900 - top-4-accuracy: 0.6100 - val_loss: 2.1313 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.4000\n",
      "Epoch 7/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 2.0879 - accuracy: 0.1600 - top-4-accuracy: 0.5800 - val_loss: 2.1318 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.4000\n",
      "Epoch 8/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 2.0420 - accuracy: 0.1700 - top-4-accuracy: 0.6400 - val_loss: 2.1267 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 9/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 2.0530 - accuracy: 0.1700 - top-4-accuracy: 0.5900 - val_loss: 2.1198 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 10/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 2.0967 - accuracy: 0.1700 - top-4-accuracy: 0.5300 - val_loss: 2.1162 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 11/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 2.0964 - accuracy: 0.1200 - top-4-accuracy: 0.6000 - val_loss: 2.1151 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 12/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 2.0718 - accuracy: 0.1800 - top-4-accuracy: 0.5900 - val_loss: 2.1205 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 13/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 2.0906 - accuracy: 0.1900 - top-4-accuracy: 0.5000 - val_loss: 2.1237 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 14/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 2.0709 - accuracy: 0.1500 - top-4-accuracy: 0.5600 - val_loss: 2.1270 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 15/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 2.1242 - accuracy: 0.1800 - top-4-accuracy: 0.5300 - val_loss: 2.1317 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 16/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 2.1315 - accuracy: 0.1400 - top-4-accuracy: 0.5400 - val_loss: 2.1350 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 17/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 2.0750 - accuracy: 0.1700 - top-4-accuracy: 0.5500 - val_loss: 2.1411 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 18/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 2.1092 - accuracy: 0.0900 - top-4-accuracy: 0.5500 - val_loss: 2.1440 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.4000\n",
      "Epoch 19/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 2.0878 - accuracy: 0.1400 - top-4-accuracy: 0.5600 - val_loss: 2.1353 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.4000\n",
      "Epoch 20/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 2.1156 - accuracy: 0.1900 - top-4-accuracy: 0.5400 - val_loss: 2.1231 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 21/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 2.1211 - accuracy: 0.1600 - top-4-accuracy: 0.5100 - val_loss: 2.1104 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 22/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 2.1054 - accuracy: 0.1800 - top-4-accuracy: 0.5100 - val_loss: 2.1035 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 23/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 2.0638 - accuracy: 0.1600 - top-4-accuracy: 0.5300 - val_loss: 2.1055 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 24/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 2.1546 - accuracy: 0.1800 - top-4-accuracy: 0.5100 - val_loss: 2.1063 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 25/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 2.0983 - accuracy: 0.1400 - top-4-accuracy: 0.5900 - val_loss: 2.1002 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 26/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 2.0430 - accuracy: 0.2500 - top-4-accuracy: 0.5900 - val_loss: 2.0927 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 27/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 2.0724 - accuracy: 0.1700 - top-4-accuracy: 0.5400 - val_loss: 2.0925 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 28/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 2.0978 - accuracy: 0.1800 - top-4-accuracy: 0.5700 - val_loss: 2.0956 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 29/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 2.1502 - accuracy: 0.1200 - top-4-accuracy: 0.5100 - val_loss: 2.1003 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 30/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 2.1496 - accuracy: 0.0900 - top-4-accuracy: 0.5000 - val_loss: 2.0956 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 31/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 2.0864 - accuracy: 0.1500 - top-4-accuracy: 0.6200 - val_loss: 2.1000 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 32/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 2.1049 - accuracy: 0.1000 - top-4-accuracy: 0.5500 - val_loss: 2.1083 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 33/1000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 2.0877 - accuracy: 0.1500 - top-4-accuracy: 0.5800 - val_loss: 2.1151 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 34/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 2.0438 - accuracy: 0.2000 - top-4-accuracy: 0.6300 - val_loss: 2.1304 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.4000\n",
      "Epoch 35/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 2.1228 - accuracy: 0.2000 - top-4-accuracy: 0.5400 - val_loss: 2.1442 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 36/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 2.1768 - accuracy: 0.1600 - top-4-accuracy: 0.4400 - val_loss: 2.1465 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 37/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 2.0832 - accuracy: 0.1000 - top-4-accuracy: 0.5300 - val_loss: 2.1394 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 38/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 2.0866 - accuracy: 0.1500 - top-4-accuracy: 0.5700 - val_loss: 2.1315 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 39/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 2.1315 - accuracy: 0.1800 - top-4-accuracy: 0.5300 - val_loss: 2.1216 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 40/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 2.1360 - accuracy: 0.1700 - top-4-accuracy: 0.5600 - val_loss: 2.0909 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 41/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 2.1049 - accuracy: 0.1700 - top-4-accuracy: 0.6100 - val_loss: 2.0709 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.6000\n",
      "Epoch 42/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 2.0904 - accuracy: 0.1500 - top-4-accuracy: 0.5800 - val_loss: 2.0710 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 43/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 2.1100 - accuracy: 0.1300 - top-4-accuracy: 0.5500 - val_loss: 2.0890 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 44/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 2.1566 - accuracy: 0.1300 - top-4-accuracy: 0.4200 - val_loss: 2.1096 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 45/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 2.0569 - accuracy: 0.2000 - top-4-accuracy: 0.6300 - val_loss: 2.1515 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.4000\n",
      "Epoch 46/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 2.0964 - accuracy: 0.1800 - top-4-accuracy: 0.5500 - val_loss: 2.1762 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.4000\n",
      "Epoch 47/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 2.1283 - accuracy: 0.2000 - top-4-accuracy: 0.5000 - val_loss: 2.1610 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 48/1000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 2.0834 - accuracy: 0.2100 - top-4-accuracy: 0.5900 - val_loss: 2.1388 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 49/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 2.1261 - accuracy: 0.1600 - top-4-accuracy: 0.5200 - val_loss: 2.0955 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 50/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 2.1035 - accuracy: 0.1900 - top-4-accuracy: 0.5700 - val_loss: 2.0681 - val_accuracy: 0.2000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 51/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 2.1122 - accuracy: 0.1400 - top-4-accuracy: 0.5700 - val_loss: 2.0579 - val_accuracy: 0.2000 - val_top-4-accuracy: 0.6000\n",
      "Epoch 52/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 2.1303 - accuracy: 0.1600 - top-4-accuracy: 0.5500 - val_loss: 2.0699 - val_accuracy: 0.2000 - val_top-4-accuracy: 0.6000\n",
      "Epoch 53/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 2.0690 - accuracy: 0.1600 - top-4-accuracy: 0.6100 - val_loss: 2.0989 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.6000\n",
      "Epoch 54/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 2.0446 - accuracy: 0.1700 - top-4-accuracy: 0.6400 - val_loss: 2.1365 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.4000\n",
      "Epoch 55/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 2.1346 - accuracy: 0.1300 - top-4-accuracy: 0.5000 - val_loss: 2.1609 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.4000\n",
      "Epoch 56/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 2.1407 - accuracy: 0.1500 - top-4-accuracy: 0.5600 - val_loss: 2.1787 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.4000\n",
      "Epoch 57/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 2.1196 - accuracy: 0.2300 - top-4-accuracy: 0.5900 - val_loss: 2.1550 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 58/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 2.0590 - accuracy: 0.2500 - top-4-accuracy: 0.5600 - val_loss: 2.1167 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 59/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 2.1685 - accuracy: 0.1300 - top-4-accuracy: 0.5200 - val_loss: 2.0885 - val_accuracy: 0.2000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 60/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 2.1287 - accuracy: 0.1200 - top-4-accuracy: 0.5500 - val_loss: 2.0604 - val_accuracy: 0.2000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 61/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 2.0678 - accuracy: 0.1500 - top-4-accuracy: 0.6100 - val_loss: 2.0507 - val_accuracy: 0.2000 - val_top-4-accuracy: 0.6000\n",
      "Epoch 62/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 2.0813 - accuracy: 0.2100 - top-4-accuracy: 0.5500 - val_loss: 2.0741 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.6000\n",
      "Epoch 63/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 2.1322 - accuracy: 0.2100 - top-4-accuracy: 0.5000 - val_loss: 2.1362 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 64/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 2.1230 - accuracy: 0.1500 - top-4-accuracy: 0.6100 - val_loss: 2.1631 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 65/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 2.1201 - accuracy: 0.0900 - top-4-accuracy: 0.5600 - val_loss: 2.1635 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 66/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 2.1001 - accuracy: 0.1700 - top-4-accuracy: 0.5700 - val_loss: 2.1487 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.4000\n",
      "Epoch 67/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 2.0793 - accuracy: 0.1800 - top-4-accuracy: 0.5900 - val_loss: 2.1243 - val_accuracy: 0.2000 - val_top-4-accuracy: 0.4000\n",
      "Epoch 68/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 2.1660 - accuracy: 0.0900 - top-4-accuracy: 0.5300 - val_loss: 2.1052 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.4000\n",
      "Epoch 69/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 2.0968 - accuracy: 0.1400 - top-4-accuracy: 0.5800 - val_loss: 2.1009 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.6000\n",
      "Epoch 70/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 2.1441 - accuracy: 0.1600 - top-4-accuracy: 0.5600 - val_loss: 2.1138 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.6000\n",
      "Epoch 71/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 2.0923 - accuracy: 0.2100 - top-4-accuracy: 0.5500 - val_loss: 2.1074 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.6000\n",
      "Epoch 72/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 2.1110 - accuracy: 0.1200 - top-4-accuracy: 0.4500 - val_loss: 2.1157 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 73/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 2.0932 - accuracy: 0.1800 - top-4-accuracy: 0.5900 - val_loss: 2.1208 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 74/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 2.1612 - accuracy: 0.1400 - top-4-accuracy: 0.5100 - val_loss: 2.1372 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 75/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 2.1764 - accuracy: 0.1600 - top-4-accuracy: 0.5300 - val_loss: 2.1471 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 76/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 2.1262 - accuracy: 0.1700 - top-4-accuracy: 0.5100 - val_loss: 2.1379 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 77/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 2.1457 - accuracy: 0.1400 - top-4-accuracy: 0.5300 - val_loss: 2.1275 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 78/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 2.1053 - accuracy: 0.1300 - top-4-accuracy: 0.5500 - val_loss: 2.1061 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 79/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 2.2195 - accuracy: 0.1300 - top-4-accuracy: 0.4200 - val_loss: 2.0822 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.6000\n",
      "Epoch 80/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 2.0993 - accuracy: 0.1100 - top-4-accuracy: 0.4900 - val_loss: 2.0896 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 81/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 2.1788 - accuracy: 0.1800 - top-4-accuracy: 0.4800 - val_loss: 2.1219 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 82/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 2.1181 - accuracy: 0.1200 - top-4-accuracy: 0.5000 - val_loss: 2.1542 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 83/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 2.1619 - accuracy: 0.1300 - top-4-accuracy: 0.5300 - val_loss: 2.1739 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 84/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 2.0720 - accuracy: 0.1600 - top-4-accuracy: 0.5800 - val_loss: 2.1527 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 85/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 2.1117 - accuracy: 0.1600 - top-4-accuracy: 0.5400 - val_loss: 2.1396 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 86/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 2.1841 - accuracy: 0.1400 - top-4-accuracy: 0.4800 - val_loss: 2.1169 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 87/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 2.1685 - accuracy: 0.1400 - top-4-accuracy: 0.5200 - val_loss: 2.0947 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.6000\n",
      "Epoch 88/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 2.1522 - accuracy: 0.1700 - top-4-accuracy: 0.5300 - val_loss: 2.0939 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.6000\n",
      "Epoch 89/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 2.1302 - accuracy: 0.1000 - top-4-accuracy: 0.5900 - val_loss: 2.0953 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.6000\n",
      "Epoch 90/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 2.0951 - accuracy: 0.1800 - top-4-accuracy: 0.5700 - val_loss: 2.1145 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 91/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 2.0760 - accuracy: 0.1700 - top-4-accuracy: 0.6000 - val_loss: 2.1250 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 92/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 2.1259 - accuracy: 0.1400 - top-4-accuracy: 0.5000 - val_loss: 2.1385 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 93/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 2.0634 - accuracy: 0.2100 - top-4-accuracy: 0.6100 - val_loss: 2.1343 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 94/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 2.0576 - accuracy: 0.2000 - top-4-accuracy: 0.6200 - val_loss: 2.1357 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 95/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 2.0807 - accuracy: 0.1900 - top-4-accuracy: 0.5700 - val_loss: 2.1485 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.4000\n",
      "Epoch 96/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 2.1363 - accuracy: 0.1400 - top-4-accuracy: 0.5400 - val_loss: 2.1533 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.4000\n",
      "Epoch 97/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 2.0985 - accuracy: 0.2200 - top-4-accuracy: 0.5000 - val_loss: 2.1327 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 98/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 2.1308 - accuracy: 0.1000 - top-4-accuracy: 0.6200 - val_loss: 2.1163 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 99/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 2.1147 - accuracy: 0.1400 - top-4-accuracy: 0.5400 - val_loss: 2.1280 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 100/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 2.0903 - accuracy: 0.1400 - top-4-accuracy: 0.5800 - val_loss: 2.1254 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 101/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 2.0935 - accuracy: 0.1400 - top-4-accuracy: 0.6000 - val_loss: 2.1108 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 102/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 2.1064 - accuracy: 0.1700 - top-4-accuracy: 0.5400 - val_loss: 2.1101 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 103/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 2.0974 - accuracy: 0.1700 - top-4-accuracy: 0.6000 - val_loss: 2.1164 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.6000\n",
      "Epoch 104/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 2.0774 - accuracy: 0.2300 - top-4-accuracy: 0.6300 - val_loss: 2.1140 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.6000\n",
      "Epoch 105/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 2.0753 - accuracy: 0.1700 - top-4-accuracy: 0.5500 - val_loss: 2.1240 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 106/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 2.1320 - accuracy: 0.1300 - top-4-accuracy: 0.5300 - val_loss: 2.1420 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 107/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 2.1204 - accuracy: 0.1500 - top-4-accuracy: 0.5200 - val_loss: 2.1236 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 108/1000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 2.1167 - accuracy: 0.1500 - top-4-accuracy: 0.5300 - val_loss: 2.1174 - val_accuracy: 0.2000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 109/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 2.0805 - accuracy: 0.1900 - top-4-accuracy: 0.5700 - val_loss: 2.1353 - val_accuracy: 0.2000 - val_top-4-accuracy: 0.4000\n",
      "Epoch 110/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 2.0853 - accuracy: 0.1100 - top-4-accuracy: 0.5700 - val_loss: 2.1512 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 111/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 2.1107 - accuracy: 0.1600 - top-4-accuracy: 0.6400 - val_loss: 2.1725 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 112/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 2.0991 - accuracy: 0.1700 - top-4-accuracy: 0.6000 - val_loss: 2.1710 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 113/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 2.1170 - accuracy: 0.1600 - top-4-accuracy: 0.5700 - val_loss: 2.1299 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 114/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 2.1115 - accuracy: 0.1600 - top-4-accuracy: 0.5700 - val_loss: 2.0814 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 115/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 2.1021 - accuracy: 0.1200 - top-4-accuracy: 0.5600 - val_loss: 2.0639 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 116/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 2.1070 - accuracy: 0.1600 - top-4-accuracy: 0.5100 - val_loss: 2.0628 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.6000\n",
      "Epoch 117/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 2.1281 - accuracy: 0.1200 - top-4-accuracy: 0.5100 - val_loss: 2.0853 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.6000\n",
      "Epoch 118/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 2.0639 - accuracy: 0.1200 - top-4-accuracy: 0.6200 - val_loss: 2.1238 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 119/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 2.1233 - accuracy: 0.1500 - top-4-accuracy: 0.5400 - val_loss: 2.1666 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 120/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 2.0869 - accuracy: 0.1900 - top-4-accuracy: 0.5700 - val_loss: 2.1908 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 121/1000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 2.1240 - accuracy: 0.1500 - top-4-accuracy: 0.5500 - val_loss: 2.1918 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 122/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 2.1140 - accuracy: 0.1700 - top-4-accuracy: 0.5100 - val_loss: 2.1738 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 123/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 2.1144 - accuracy: 0.1600 - top-4-accuracy: 0.4900 - val_loss: 2.1324 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 124/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 2.0701 - accuracy: 0.1900 - top-4-accuracy: 0.6000 - val_loss: 2.0977 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 125/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 2.1032 - accuracy: 0.1300 - top-4-accuracy: 0.5900 - val_loss: 2.0846 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 126/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 2.0680 - accuracy: 0.2100 - top-4-accuracy: 0.6000 - val_loss: 2.0871 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 127/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 2.1006 - accuracy: 0.1700 - top-4-accuracy: 0.5700 - val_loss: 2.0949 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 128/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 2.0835 - accuracy: 0.1700 - top-4-accuracy: 0.6000 - val_loss: 2.1072 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 129/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 2.0854 - accuracy: 0.1500 - top-4-accuracy: 0.5300 - val_loss: 2.1276 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.4000\n",
      "Epoch 130/1000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 2.1102 - accuracy: 0.1500 - top-4-accuracy: 0.5100 - val_loss: 2.1477 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 131/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 2.0559 - accuracy: 0.1800 - top-4-accuracy: 0.6500 - val_loss: 2.1561 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 132/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 2.1088 - accuracy: 0.2000 - top-4-accuracy: 0.4800 - val_loss: 2.1548 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 133/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 2.0901 - accuracy: 0.1800 - top-4-accuracy: 0.6000 - val_loss: 2.1539 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 134/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 2.1051 - accuracy: 0.1800 - top-4-accuracy: 0.6000 - val_loss: 2.1558 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 135/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 2.0418 - accuracy: 0.1800 - top-4-accuracy: 0.6200 - val_loss: 2.1526 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 136/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 2.0951 - accuracy: 0.1800 - top-4-accuracy: 0.5900 - val_loss: 2.1475 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.4000\n",
      "Epoch 137/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 2.0814 - accuracy: 0.1700 - top-4-accuracy: 0.6000 - val_loss: 2.1281 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.4000\n",
      "Epoch 138/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 2.0475 - accuracy: 0.1800 - top-4-accuracy: 0.5800 - val_loss: 2.1069 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.4000\n",
      "Epoch 139/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 2.0888 - accuracy: 0.1200 - top-4-accuracy: 0.5400 - val_loss: 2.0939 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.4000\n",
      "Epoch 140/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 2.1100 - accuracy: 0.1300 - top-4-accuracy: 0.4900 - val_loss: 2.0897 - val_accuracy: 0.2000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 141/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 2.1090 - accuracy: 0.1300 - top-4-accuracy: 0.5500 - val_loss: 2.0892 - val_accuracy: 0.2000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 142/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 2.0437 - accuracy: 0.1700 - top-4-accuracy: 0.5900 - val_loss: 2.0953 - val_accuracy: 0.3000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 143/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 2.1389 - accuracy: 0.0900 - top-4-accuracy: 0.5300 - val_loss: 2.1101 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 144/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 2.1104 - accuracy: 0.1600 - top-4-accuracy: 0.5300 - val_loss: 2.1159 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 145/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 2.1100 - accuracy: 0.1500 - top-4-accuracy: 0.5100 - val_loss: 2.1186 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 146/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 2.1225 - accuracy: 0.1500 - top-4-accuracy: 0.5600 - val_loss: 2.1170 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 147/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 2.0743 - accuracy: 0.2100 - top-4-accuracy: 0.5600 - val_loss: 2.1186 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.6000\n",
      "Epoch 148/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 2.0628 - accuracy: 0.1600 - top-4-accuracy: 0.5900 - val_loss: 2.1284 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 149/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 2.0998 - accuracy: 0.1700 - top-4-accuracy: 0.5600 - val_loss: 2.1350 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.4000\n",
      "Epoch 150/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 2.0769 - accuracy: 0.1500 - top-4-accuracy: 0.6000 - val_loss: 2.1362 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 151/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 2.1079 - accuracy: 0.1500 - top-4-accuracy: 0.4600 - val_loss: 2.1275 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 152/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 2.0778 - accuracy: 0.1400 - top-4-accuracy: 0.4900 - val_loss: 2.1160 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 153/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 2.1175 - accuracy: 0.1100 - top-4-accuracy: 0.5400 - val_loss: 2.1122 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 154/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 2.1025 - accuracy: 0.2000 - top-4-accuracy: 0.5100 - val_loss: 2.1096 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 155/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 2.1219 - accuracy: 0.1500 - top-4-accuracy: 0.4700 - val_loss: 2.1034 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 156/1000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 2.0450 - accuracy: 0.2100 - top-4-accuracy: 0.6100 - val_loss: 2.1049 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 157/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 2.0570 - accuracy: 0.1600 - top-4-accuracy: 0.5900 - val_loss: 2.1167 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 158/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 2.0637 - accuracy: 0.1800 - top-4-accuracy: 0.5900 - val_loss: 2.1206 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 159/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 2.0736 - accuracy: 0.1100 - top-4-accuracy: 0.5700 - val_loss: 2.1265 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 160/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 2.0830 - accuracy: 0.1400 - top-4-accuracy: 0.5800 - val_loss: 2.1280 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 161/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 2.0664 - accuracy: 0.1500 - top-4-accuracy: 0.6000 - val_loss: 2.1208 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 162/1000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 2.0619 - accuracy: 0.1900 - top-4-accuracy: 0.5600 - val_loss: 2.1093 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 163/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 2.0923 - accuracy: 0.1700 - top-4-accuracy: 0.5400 - val_loss: 2.0939 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 164/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 2.1080 - accuracy: 0.1800 - top-4-accuracy: 0.5200 - val_loss: 2.0895 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 165/1000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 2.0951 - accuracy: 0.2000 - top-4-accuracy: 0.5700 - val_loss: 2.0990 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 166/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 2.1048 - accuracy: 0.1500 - top-4-accuracy: 0.5200 - val_loss: 2.1051 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 167/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 2.1076 - accuracy: 0.1200 - top-4-accuracy: 0.5500 - val_loss: 2.1104 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 168/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 2.0930 - accuracy: 0.1200 - top-4-accuracy: 0.5000 - val_loss: 2.1134 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 169/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 2.0898 - accuracy: 0.1200 - top-4-accuracy: 0.5500 - val_loss: 2.1165 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 170/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 2.0706 - accuracy: 0.1800 - top-4-accuracy: 0.5600 - val_loss: 2.1170 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 171/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 2.0943 - accuracy: 0.1800 - top-4-accuracy: 0.5100 - val_loss: 2.1148 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 172/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 2.0943 - accuracy: 0.1100 - top-4-accuracy: 0.5600 - val_loss: 2.1077 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.6000\n",
      "Epoch 173/1000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 2.0758 - accuracy: 0.1500 - top-4-accuracy: 0.5800 - val_loss: 2.1073 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 174/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 2.0932 - accuracy: 0.2100 - top-4-accuracy: 0.5300 - val_loss: 2.1093 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 175/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 2.0437 - accuracy: 0.2200 - top-4-accuracy: 0.5900 - val_loss: 2.1155 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 176/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 2.0985 - accuracy: 0.1200 - top-4-accuracy: 0.5500 - val_loss: 2.1263 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 177/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 2.0467 - accuracy: 0.2100 - top-4-accuracy: 0.5600 - val_loss: 2.1321 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 178/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 2.0377 - accuracy: 0.1800 - top-4-accuracy: 0.6200 - val_loss: 2.1380 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 179/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 2.0959 - accuracy: 0.1500 - top-4-accuracy: 0.5400 - val_loss: 2.1431 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 180/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 2.0321 - accuracy: 0.2200 - top-4-accuracy: 0.5700 - val_loss: 2.1387 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 181/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 2.0589 - accuracy: 0.2000 - top-4-accuracy: 0.6100 - val_loss: 2.1347 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 182/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 2.0940 - accuracy: 0.1700 - top-4-accuracy: 0.5600 - val_loss: 2.1285 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 183/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 2.1266 - accuracy: 0.1400 - top-4-accuracy: 0.4700 - val_loss: 2.1198 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 184/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 2.0670 - accuracy: 0.1300 - top-4-accuracy: 0.5800 - val_loss: 2.1108 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 185/1000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 2.0820 - accuracy: 0.1100 - top-4-accuracy: 0.6000 - val_loss: 2.1109 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 186/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 2.0875 - accuracy: 0.1400 - top-4-accuracy: 0.5000 - val_loss: 2.1163 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 187/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 2.0493 - accuracy: 0.1800 - top-4-accuracy: 0.6000 - val_loss: 2.1187 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 188/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 2.0553 - accuracy: 0.1500 - top-4-accuracy: 0.6500 - val_loss: 2.1228 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 189/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 2.1224 - accuracy: 0.1800 - top-4-accuracy: 0.5200 - val_loss: 2.1289 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 190/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 2.0477 - accuracy: 0.1600 - top-4-accuracy: 0.6000 - val_loss: 2.1378 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 191/1000\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 2.1018 - accuracy: 0.1600 - top-4-accuracy: 0.4900 - val_loss: 2.1474 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.4000\n",
      "Epoch 192/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 2.0987 - accuracy: 0.1800 - top-4-accuracy: 0.5100 - val_loss: 2.1538 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.4000\n",
      "Epoch 193/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 2.0678 - accuracy: 0.1200 - top-4-accuracy: 0.5400 - val_loss: 2.1492 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.4000\n",
      "Epoch 194/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 2.0798 - accuracy: 0.1700 - top-4-accuracy: 0.5400 - val_loss: 2.1428 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.4000\n",
      "Epoch 195/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 2.0910 - accuracy: 0.1800 - top-4-accuracy: 0.5500 - val_loss: 2.1375 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.4000\n",
      "Epoch 196/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 2.0812 - accuracy: 0.1200 - top-4-accuracy: 0.5500 - val_loss: 2.1333 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 197/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 2.0843 - accuracy: 0.2100 - top-4-accuracy: 0.5200 - val_loss: 2.1251 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 198/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 2.0731 - accuracy: 0.2200 - top-4-accuracy: 0.5700 - val_loss: 2.1153 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 199/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 2.0957 - accuracy: 0.2000 - top-4-accuracy: 0.6000 - val_loss: 2.1116 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 200/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 2.0774 - accuracy: 0.1300 - top-4-accuracy: 0.5200 - val_loss: 2.1157 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 201/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 2.1094 - accuracy: 0.1600 - top-4-accuracy: 0.4900 - val_loss: 2.1187 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 202/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 2.0879 - accuracy: 0.1500 - top-4-accuracy: 0.5400 - val_loss: 2.1237 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 203/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 2.0513 - accuracy: 0.1800 - top-4-accuracy: 0.6000 - val_loss: 2.1306 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 204/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 2.0752 - accuracy: 0.1500 - top-4-accuracy: 0.6000 - val_loss: 2.1316 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 205/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 2.0835 - accuracy: 0.1600 - top-4-accuracy: 0.5900 - val_loss: 2.1287 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 206/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 2.0738 - accuracy: 0.1700 - top-4-accuracy: 0.5500 - val_loss: 2.1203 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 207/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 2.0562 - accuracy: 0.1600 - top-4-accuracy: 0.6400 - val_loss: 2.1159 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 208/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 2.0118 - accuracy: 0.2500 - top-4-accuracy: 0.6200 - val_loss: 2.1114 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 209/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 2.1035 - accuracy: 0.1400 - top-4-accuracy: 0.5400 - val_loss: 2.1107 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 210/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 2.0912 - accuracy: 0.1800 - top-4-accuracy: 0.5100 - val_loss: 2.1175 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 211/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 2.0616 - accuracy: 0.1500 - top-4-accuracy: 0.5900 - val_loss: 2.1261 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.4000\n",
      "Epoch 212/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 2.0588 - accuracy: 0.2300 - top-4-accuracy: 0.5800 - val_loss: 2.1353 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.4000\n",
      "Epoch 213/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 2.0791 - accuracy: 0.2000 - top-4-accuracy: 0.5900 - val_loss: 2.1446 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 214/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 2.0993 - accuracy: 0.1400 - top-4-accuracy: 0.5500 - val_loss: 2.1532 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 215/1000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 2.0619 - accuracy: 0.1400 - top-4-accuracy: 0.5800 - val_loss: 2.1592 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 216/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 2.0469 - accuracy: 0.1900 - top-4-accuracy: 0.6200 - val_loss: 2.1561 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 217/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 2.0449 - accuracy: 0.1200 - top-4-accuracy: 0.6100 - val_loss: 2.1515 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 218/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 2.0336 - accuracy: 0.1500 - top-4-accuracy: 0.6000 - val_loss: 2.1451 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 219/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 2.0341 - accuracy: 0.1500 - top-4-accuracy: 0.6400 - val_loss: 2.1404 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 220/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 2.0725 - accuracy: 0.2000 - top-4-accuracy: 0.5700 - val_loss: 2.1339 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 221/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 2.0696 - accuracy: 0.2100 - top-4-accuracy: 0.5500 - val_loss: 2.1308 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 222/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 2.0487 - accuracy: 0.1800 - top-4-accuracy: 0.5700 - val_loss: 2.1316 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.4000\n",
      "Epoch 223/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 2.0620 - accuracy: 0.1700 - top-4-accuracy: 0.6300 - val_loss: 2.1339 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.4000\n",
      "Epoch 224/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 2.0991 - accuracy: 0.1200 - top-4-accuracy: 0.6500 - val_loss: 2.1341 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 225/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 2.0627 - accuracy: 0.1800 - top-4-accuracy: 0.5500 - val_loss: 2.1303 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 226/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 2.0670 - accuracy: 0.2200 - top-4-accuracy: 0.5600 - val_loss: 2.1294 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 227/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 2.1141 - accuracy: 0.1900 - top-4-accuracy: 0.5500 - val_loss: 2.1329 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 228/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 2.0663 - accuracy: 0.1400 - top-4-accuracy: 0.5500 - val_loss: 2.1389 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 229/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 2.0561 - accuracy: 0.1700 - top-4-accuracy: 0.5800 - val_loss: 2.1438 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 230/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 2.0729 - accuracy: 0.1500 - top-4-accuracy: 0.5500 - val_loss: 2.1439 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 231/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 2.0686 - accuracy: 0.1700 - top-4-accuracy: 0.6300 - val_loss: 2.1364 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 232/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 2.0462 - accuracy: 0.1300 - top-4-accuracy: 0.6200 - val_loss: 2.1325 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 233/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 2.0575 - accuracy: 0.1700 - top-4-accuracy: 0.5500 - val_loss: 2.1263 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 234/1000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 2.0852 - accuracy: 0.1500 - top-4-accuracy: 0.5800 - val_loss: 2.1207 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 235/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 2.0622 - accuracy: 0.2100 - top-4-accuracy: 0.5600 - val_loss: 2.1243 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 236/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 2.0673 - accuracy: 0.1800 - top-4-accuracy: 0.6100 - val_loss: 2.1318 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 237/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 2.0566 - accuracy: 0.1700 - top-4-accuracy: 0.6200 - val_loss: 2.1273 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 238/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 2.0691 - accuracy: 0.1600 - top-4-accuracy: 0.5700 - val_loss: 2.1209 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 239/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 2.0860 - accuracy: 0.1900 - top-4-accuracy: 0.5900 - val_loss: 2.1104 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 240/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 2.0512 - accuracy: 0.1800 - top-4-accuracy: 0.5700 - val_loss: 2.1062 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 241/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 2.0520 - accuracy: 0.2100 - top-4-accuracy: 0.5400 - val_loss: 2.1073 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 242/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 2.1014 - accuracy: 0.1600 - top-4-accuracy: 0.5600 - val_loss: 2.1044 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 243/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 2.1197 - accuracy: 0.1300 - top-4-accuracy: 0.5100 - val_loss: 2.1067 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 244/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 2.0740 - accuracy: 0.1600 - top-4-accuracy: 0.6000 - val_loss: 2.1122 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.4000\n",
      "Epoch 245/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 2.0743 - accuracy: 0.2400 - top-4-accuracy: 0.5100 - val_loss: 2.1200 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 246/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 2.1058 - accuracy: 0.0700 - top-4-accuracy: 0.5500 - val_loss: 2.1285 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 247/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 2.0820 - accuracy: 0.1700 - top-4-accuracy: 0.5600 - val_loss: 2.1353 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.4000\n",
      "Epoch 248/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 2.0962 - accuracy: 0.1600 - top-4-accuracy: 0.5900 - val_loss: 2.1394 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 249/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 2.0876 - accuracy: 0.2000 - top-4-accuracy: 0.5600 - val_loss: 2.1399 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 250/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 2.0858 - accuracy: 0.1300 - top-4-accuracy: 0.5700 - val_loss: 2.1377 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 251/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 2.0639 - accuracy: 0.1700 - top-4-accuracy: 0.5900 - val_loss: 2.1297 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 252/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 2.0772 - accuracy: 0.1800 - top-4-accuracy: 0.5400 - val_loss: 2.1232 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 253/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 2.0978 - accuracy: 0.1900 - top-4-accuracy: 0.5700 - val_loss: 2.1176 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 254/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 2.0634 - accuracy: 0.1900 - top-4-accuracy: 0.5600 - val_loss: 2.1130 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 255/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 2.0426 - accuracy: 0.2000 - top-4-accuracy: 0.6300 - val_loss: 2.1081 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 256/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 1.9986 - accuracy: 0.2200 - top-4-accuracy: 0.6900 - val_loss: 2.1092 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 257/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 2.0468 - accuracy: 0.1900 - top-4-accuracy: 0.5500 - val_loss: 2.1133 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 258/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 2.0783 - accuracy: 0.1100 - top-4-accuracy: 0.5500 - val_loss: 2.1179 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 259/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 2.0657 - accuracy: 0.1600 - top-4-accuracy: 0.5700 - val_loss: 2.1237 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 260/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 2.0655 - accuracy: 0.1900 - top-4-accuracy: 0.5700 - val_loss: 2.1226 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 261/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 2.0719 - accuracy: 0.1600 - top-4-accuracy: 0.5500 - val_loss: 2.1210 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 262/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 2.0751 - accuracy: 0.1600 - top-4-accuracy: 0.5500 - val_loss: 2.1261 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 263/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 2.0497 - accuracy: 0.1700 - top-4-accuracy: 0.6600 - val_loss: 2.1292 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 264/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 2.0398 - accuracy: 0.1800 - top-4-accuracy: 0.6000 - val_loss: 2.1318 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 265/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 2.0706 - accuracy: 0.2100 - top-4-accuracy: 0.5500 - val_loss: 2.1349 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 266/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 2.1144 - accuracy: 0.1400 - top-4-accuracy: 0.5500 - val_loss: 2.1348 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 267/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 2.0835 - accuracy: 0.1500 - top-4-accuracy: 0.6100 - val_loss: 2.1380 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 268/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 2.0483 - accuracy: 0.2100 - top-4-accuracy: 0.6000 - val_loss: 2.1401 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 269/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 2.0808 - accuracy: 0.1800 - top-4-accuracy: 0.5800 - val_loss: 2.1390 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 270/1000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 2.0826 - accuracy: 0.1500 - top-4-accuracy: 0.5600 - val_loss: 2.1381 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 271/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 2.0888 - accuracy: 0.1400 - top-4-accuracy: 0.5200 - val_loss: 2.1375 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 272/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 2.0707 - accuracy: 0.1900 - top-4-accuracy: 0.5400 - val_loss: 2.1323 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 273/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 2.0962 - accuracy: 0.1900 - top-4-accuracy: 0.5100 - val_loss: 2.1308 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 274/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 2.0655 - accuracy: 0.1800 - top-4-accuracy: 0.6100 - val_loss: 2.1318 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 275/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 2.0602 - accuracy: 0.2200 - top-4-accuracy: 0.6100 - val_loss: 2.1279 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 276/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 2.0318 - accuracy: 0.2100 - top-4-accuracy: 0.6200 - val_loss: 2.1219 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 277/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 2.0759 - accuracy: 0.1700 - top-4-accuracy: 0.5200 - val_loss: 2.1202 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 278/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 2.0793 - accuracy: 0.2100 - top-4-accuracy: 0.5900 - val_loss: 2.1171 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 279/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 2.1198 - accuracy: 0.1900 - top-4-accuracy: 0.5200 - val_loss: 2.1157 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 280/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 2.0692 - accuracy: 0.1700 - top-4-accuracy: 0.5800 - val_loss: 2.1156 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 281/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 2.0686 - accuracy: 0.1700 - top-4-accuracy: 0.5500 - val_loss: 2.1115 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 282/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 2.0632 - accuracy: 0.1500 - top-4-accuracy: 0.6100 - val_loss: 2.1121 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 283/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 2.0751 - accuracy: 0.1700 - top-4-accuracy: 0.5700 - val_loss: 2.1116 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 284/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 2.0634 - accuracy: 0.1800 - top-4-accuracy: 0.5700 - val_loss: 2.1116 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 285/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 2.0471 - accuracy: 0.1800 - top-4-accuracy: 0.5700 - val_loss: 2.1168 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 286/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 2.0469 - accuracy: 0.1300 - top-4-accuracy: 0.6400 - val_loss: 2.1210 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 287/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 2.0578 - accuracy: 0.1900 - top-4-accuracy: 0.5600 - val_loss: 2.1280 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 288/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 2.0577 - accuracy: 0.2100 - top-4-accuracy: 0.5900 - val_loss: 2.1351 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 289/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 2.1088 - accuracy: 0.2000 - top-4-accuracy: 0.4800 - val_loss: 2.1358 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 290/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 2.0812 - accuracy: 0.1800 - top-4-accuracy: 0.5200 - val_loss: 2.1311 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 291/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 2.0568 - accuracy: 0.1800 - top-4-accuracy: 0.5900 - val_loss: 2.1250 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 292/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 2.0317 - accuracy: 0.2400 - top-4-accuracy: 0.5900 - val_loss: 2.1234 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 293/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 2.0757 - accuracy: 0.1900 - top-4-accuracy: 0.5500 - val_loss: 2.1225 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 294/1000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 2.0517 - accuracy: 0.1400 - top-4-accuracy: 0.6000 - val_loss: 2.1275 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 295/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 2.0666 - accuracy: 0.1800 - top-4-accuracy: 0.6500 - val_loss: 2.1341 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 296/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 2.0578 - accuracy: 0.1700 - top-4-accuracy: 0.5700 - val_loss: 2.1383 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 297/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 2.0665 - accuracy: 0.1700 - top-4-accuracy: 0.6100 - val_loss: 2.1378 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 298/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 2.1024 - accuracy: 0.1800 - top-4-accuracy: 0.5700 - val_loss: 2.1347 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 299/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 2.0615 - accuracy: 0.2000 - top-4-accuracy: 0.5700 - val_loss: 2.1339 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 300/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 2.0486 - accuracy: 0.1900 - top-4-accuracy: 0.5900 - val_loss: 2.1319 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 301/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 2.0587 - accuracy: 0.2000 - top-4-accuracy: 0.6000 - val_loss: 2.1290 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 302/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 2.0736 - accuracy: 0.2000 - top-4-accuracy: 0.5600 - val_loss: 2.1230 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 303/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 2.0865 - accuracy: 0.1700 - top-4-accuracy: 0.5400 - val_loss: 2.1150 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 304/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 2.0827 - accuracy: 0.1700 - top-4-accuracy: 0.5600 - val_loss: 2.1105 - val_accuracy: 0.1000 - val_top-4-accuracy: 0.5000\n",
      "Epoch 305/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.0936 - accuracy: 0.1562 - top-4-accuracy: 0.5781"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5442/661151465.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/device:GPU:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mvit_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_vit_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvit_classifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_5442/1474480451.py\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     21\u001b[0m     )\n\u001b[1;32m     22\u001b[0m     \u001b[0mscheduler_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLearningRateScheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     history = model.fit(\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3037\u001b[0m       (graph_function,\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3039\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1963\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.device('/device:GPU:0'):\n",
    "    vit_classifier = create_vit_classifier()\n",
    "    history = run_experiment(vit_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-james",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional n epochs? - will overwrite history...\n",
    "# num_epochs = 20\n",
    "# history = run_experiment(vit_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4111b591-3160-4ef3-92b3-c46c42662e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:1'):\n",
    "    score = vit_classifier.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-aviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)  \n",
    "  \n",
    "# summarize history for accuracy  \n",
    "   \n",
    "plt.subplot(211)  \n",
    "plt.plot(history.history['accuracy'])  \n",
    "plt.plot(history.history['val_accuracy'])  \n",
    "plt.title('model accuracy')  \n",
    "plt.ylabel('accuracy')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'test'], loc='upper left')  \n",
    "   \n",
    "# summarize history for loss  \n",
    "   \n",
    "plt.subplot(212)  \n",
    "plt.plot(history.history['loss'])  \n",
    "plt.plot(history.history['val_loss'])  \n",
    "plt.title('model loss')  \n",
    "plt.ylabel('loss')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'test'], loc='upper left') \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hearing-charm",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:1'):\n",
    "    history = run_experiment(vit_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-merchandise",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:1'):\n",
    "    score = vit_classifier.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "private-legend",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_classifier.summary()\n",
    "keras.utils.plot_model(vit_classifier,\n",
    "                       to_file=\"vit-cifar.png\",\n",
    "                       show_shapes=True,\n",
    "                       expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trained-middle",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)  \n",
    "  \n",
    "# summarize history for accuracy  \n",
    "   \n",
    "plt.subplot(211)  \n",
    "plt.plot(history.history['accuracy'])  \n",
    "plt.plot(history.history['val_accuracy'])  \n",
    "plt.title('model accuracy')  \n",
    "plt.ylabel('accuracy')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'test'], loc='upper left')  \n",
    "   \n",
    "# summarize history for loss  \n",
    "   \n",
    "plt.subplot(212)  \n",
    "plt.plot(history.history['loss'])  \n",
    "plt.plot(history.history['val_loss'])  \n",
    "plt.title('model loss')  \n",
    "plt.ylabel('loss')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'test'], loc='upper left') \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6db668d-57d8-4fc4-8f47-c6173f426b41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
