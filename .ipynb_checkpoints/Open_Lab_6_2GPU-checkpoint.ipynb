{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Lab 6 - Convolution Neural Networks\n",
    "## CSCI 4850-5850 - Neural Networks\n",
    "### Due: Feb. 28 @ 11:00pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by loading some of the tools we will need to this job. We will utilize Keras for building/training our neural network and matplotlib for plotting and visualization, and numpy for *everything* (period)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "%matplotlib inline\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "Let's use one of the most common image data sets used to benchmark convolution network implementations: MNIST. We explored it some in other introductions, so I'll let it speak for itself this time around. Please see the Single- and Multi-layer tutorials before attempting this assignment if needed: [Link](https://github.com/CSCI4850/notebook-examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "display(x_train.shape)\n",
    "display(y_train.shape)\n",
    "display(x_test.shape)\n",
    "display(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Standardize the -input- data between 0.0-1.0 (real)\n",
    "## instead of the default 0-255 (integer)\n",
    "x_train = x_train.astype('float32').reshape(x_train.shape+(1,))\n",
    "x_test = x_test.astype('float32').reshape(x_test.shape+(1,))\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# Convert class vector [0-9] to categorical assignments (one-hot)\n",
    "y_train = keras.utils.to_categorical(y_train, len(np.unique(y_train)))\n",
    "y_test = keras.utils.to_categorical(y_test, len(np.unique(y_test)))\n",
    "\n",
    "display(x_train.shape)\n",
    "display(y_train.shape)\n",
    "display(x_test.shape)\n",
    "display(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that these are greyscale images, so there is just a single intensity value for each pixel. Full color images would need to have _3_ channels (red, green, blue) for processing. Even though this is the case, Keras `Conv2D` layers expect that images have *three* dimensions (3D tensors) and so you must explicitly set this third dimension to 1 for it to accept each image as input. This is why the shapes above for the *input* patterns, $\\boldsymbol{X}$, are (n, 28, 28, 1) where each of the patterns is a 28x28x1 image. The `reshape()` function was used to add this extra dimension (wich a size of 1) to the data tensors.\n",
    "\n",
    "Let's create a convolution neural network for solving the MNIST problem. Remember, in the last lab we used a standard deep-learning or multi-layer neural network approach. We improved the performance by changing from Tanh to ReLU, and deeper networks instead of wider networks. In this way, we tried to tune the *inductive bias* of the network to form a nested solution to the problem (which isn't as easy to do with just one hidden layer). This approach performed quite well, but we are hoping to make the network even better by tuning the inductive bias even more by using the convolution approach.\n",
    "\n",
    "The convolution network will consist mostly of **filters** or **kernels** (small neural networks which will each learn to detect a single feature in the image) which can be used to *scan* the entire image to find where such features are present. We can apply additional filtering downstream in a similar way to construct groups of features, and finally a *pooling* method to detect those features in a reliable way regardless of where they are in the image. This means that we are adding an inductive bias to eliminate variance in the translation of such features from image to image. In other words, we want to create an inductive bias to develop translation-invariant representations in our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 21, 21, 64)        4160      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 128)       524416    \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               802944    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,332,810\n",
      "Trainable params: 1,332,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with strategy.scope():\n",
    "    model = keras.models.Sequential()\n",
    "    # Note the input size (there is only one channel - intensity)\n",
    "    # these images... if you are using color images, your would\n",
    "    # need to set the last dimension of the input_shape to -3-\n",
    "    # above and this would carry over into this cell...\n",
    "    model.add(keras.layers.Conv2D(64, kernel_size=(8, 8),\n",
    "              activation='relu',\n",
    "              input_shape=[x_train.shape[1],\n",
    "                           x_train.shape[2],\n",
    "                           x_train.shape[3]]))\n",
    "    model.add(keras.layers.Conv2D(128, (8, 8), activation='relu'))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(keras.layers.Dropout(0.25))\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(128, activation='relu'))\n",
    "    model.add(keras.layers.Dropout(0.5))\n",
    "    model.add(keras.layers.Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        metrics=['accuracy'])\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a pretty big network, no? Over 1 million connection weights! The convolution layers are applied twice over over the image before applying a pooling step, so the number of units *even after pooling* is quite large. Once flattened, this creates a large set of weights to the immediate dense layer which follows. The result is funneled into a smaller layer before using a softmax layer to categorize the image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting Connection Weights...\n",
    "Before moving on, let's unpack some of the output from the `summary()` function to understand what has happened here. There are *a lot of weights*, so it's worth **thinking about what they are doing and where they came from**.\n",
    "\n",
    "First, the initial `Conv2D` layer has 4160 parameters (connection weights). We selected a layer size of `64` when making the layer, so we are creating 64 different *feature detector* units which we will slide around (convolve with) the image in order to detect this feature anywhere in the image. Each unit gets its own bias weight (64 bias weights total), so there are 4096 weights left unaccounted for. For those weights, each feature detector was requested to make a `kernel_size` of 8 x 8 pixels (that's 64 weights for each unit) and so 64 x 64 = 4096. All weights for this layer are now accounted for. The same weights are used at each 8 x 8 patch in the image (overlapping and crossing) because we want each unit to become a dedicated *feature detector* during the learning process. The keras framework will automatically set up the layer to slide each of the feature detectors across all 8 x 8 pixel regions in the image as I just described.\n",
    "\n",
    "There is an additional hyper-parameter, *stride*, which could be adjusted as well to make the detector skip over a certain number of pixels each time the filter is applied. This would speed up the computation, but may miss features in the image if it is set too large. The default value is one, which is what we want, so I didn't specify *stride* at all when creating the layer. Think about this a little more, we can see that this means there are 21 x 21 unique 8 x 8 patches within this image, so that each unit calculates an activation value for each of the 441 (21 x 21) locations, but these activation patterns stay arranged in a 21 x 21 grid just like a 2D image. This means that the output for this layer is 21 x 21 x 64 in size... getting big, right? Preserving this 2D topology is important so that subsequent (downstream) layers can utilize this arrangement as well (for example, to find groups of features which are close to one another in the image).\n",
    "\n",
    "Now, for the second `Conv2D` layer, there are 524416 weights (WOW!). So, how do we account for all of those weights? Well, first we again start with the number of units requested, in this case 128. Each has a single bias weight, which leaves us with 524288 weights unaccounted for so far. There is again an 8 x 8 kernel requested at this layer, so each unit has a set of 64 weights assigned to it, but only within *one* of the 21 x 21 grids from the previous layer (and there are 64 of those 2D grids since each filter/detector was used in this way). So, 64 (kernel weights) x 128 (units) x 64 (prev units) = 524288, and all weights are now accounted for. Finally, each of these 128 units is being slid across the 21 x 21 grid from the previous layer using the 8 x 8 kernel, and there are 14 x 14 = 196 unique patches. Again, the 14 x 14 arrangement of the activation outputs is maintained for each of the 128 units to retain 2D structure. However, we have now created a double convolution (complex features detected as convolutions of simpler features).\n",
    "\n",
    "We follow up with a `MaxPooling2D` layer with a kernel size of 2 x 2. This means that we downsample the 14 x 14 grids from the previous layer with *nonoverlapping* patches of 2 x 2, and retain the maximum in this region (because it's most likely that only *one* complex feature will be detected in any of these distinct patches). This also helps with reducing the size of the activations from the previous layer by half from 14 x 14 x 128 down to 7 x 7 x 128. An additional `Dropout()` layer is also added here to encourage the network to be robust to missing features (maybe forcing different units in the convolutional layers to be somewhat redundant and tuning the inductive bias to make the network hopefully generalize a little better to new images it has yet to see).\n",
    "\n",
    "The `Flatten()` layer is designed to remove the tensorial structure of the activation (layer output) vectors at this point. Since we have 7 x 7 x 128 = 6272, it just reshapes the output from the previous layer into the flattened 1D vector of activations (1 x 6272). This allows us to stack standard `Dense` (and `Dropout`) layers from now on in the network, but the calculating the number of connection weights is now much easier. For example, after the `Flatten` layer, we have a layer of 128 units (`Dense`), so the weight matrix contains 6272 x 128 + 128 (bias weights) = 802944 connection weights total. After an additional `Dropout` layer, we have a `Dense` layer of 10 units (the output layer), so 128 x 10 + 10 (bias weights) = 1290 connection weights. Therefore, all parameters can be accounted for in the convolution and flattening process.\n",
    "\n",
    "**It's important to think through this exercise to understand how the network is wired up if you plan to use convolution nets on your particular problem...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Time\n",
    "\n",
    "OK, hold up just one second. One thing you need to understand is that this is a *big* network and there is *a lot* of data moving through it. It's not large compared to what some research groups train these days, but working on a powerful desktop (even Biosim) will still require a *several* minutes to train it up. If you complete this cycle of training, you can expect > 98% accuracy on the testing data at the end of the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:From /usr/lib/python3/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Iterator.get_next_as_optional()` instead.\n",
      "INFO:tensorflow:batch_all_reduce: 8 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 8 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "94/94 [==============================] - ETA: 0s - loss: 0.3624 - accuracy: 0.8856INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "94/94 [==============================] - 2s 23ms/step - loss: 0.3624 - accuracy: 0.8856 - val_loss: 0.0774 - val_accuracy: 0.9764\n",
      "Epoch 2/50\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.0975 - accuracy: 0.9711 - val_loss: 0.0474 - val_accuracy: 0.9856\n",
      "Epoch 3/50\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.0688 - accuracy: 0.9798 - val_loss: 0.0444 - val_accuracy: 0.9853\n",
      "Epoch 4/50\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.0532 - accuracy: 0.9840 - val_loss: 0.0422 - val_accuracy: 0.9874\n",
      "Epoch 5/50\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.0419 - accuracy: 0.9873 - val_loss: 0.0404 - val_accuracy: 0.9893\n",
      "Epoch 6/50\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.0367 - accuracy: 0.9889 - val_loss: 0.0362 - val_accuracy: 0.9898\n",
      "Epoch 7/50\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.0303 - accuracy: 0.9909 - val_loss: 0.0306 - val_accuracy: 0.9916\n",
      "Epoch 8/50\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.0288 - accuracy: 0.9910 - val_loss: 0.0290 - val_accuracy: 0.9923\n",
      "Epoch 9/50\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.0250 - accuracy: 0.9924 - val_loss: 0.0330 - val_accuracy: 0.9917\n",
      "Epoch 10/50\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.0221 - accuracy: 0.9929 - val_loss: 0.0316 - val_accuracy: 0.9918\n",
      "Epoch 11/50\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.0212 - accuracy: 0.9934 - val_loss: 0.0309 - val_accuracy: 0.9919\n",
      "Epoch 12/50\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.0193 - accuracy: 0.9939 - val_loss: 0.0340 - val_accuracy: 0.9920\n",
      "Epoch 13/50\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.0144 - accuracy: 0.9957 - val_loss: 0.0289 - val_accuracy: 0.9927\n",
      "Epoch 14/50\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.0147 - accuracy: 0.9950 - val_loss: 0.0336 - val_accuracy: 0.9915\n",
      "Epoch 15/50\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.0144 - accuracy: 0.9953 - val_loss: 0.0316 - val_accuracy: 0.9912\n",
      "Epoch 16/50\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.0151 - accuracy: 0.9952 - val_loss: 0.0336 - val_accuracy: 0.9926\n",
      "Epoch 17/50\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.0123 - accuracy: 0.9961 - val_loss: 0.0347 - val_accuracy: 0.9923\n",
      "Epoch 18/50\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.0114 - accuracy: 0.9959 - val_loss: 0.0334 - val_accuracy: 0.9926\n",
      "Epoch 19/50\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.0136 - accuracy: 0.9952 - val_loss: 0.0354 - val_accuracy: 0.9923\n",
      "Epoch 20/50\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.0093 - accuracy: 0.9967 - val_loss: 0.0317 - val_accuracy: 0.9930\n",
      "Epoch 21/50\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.0091 - accuracy: 0.9971 - val_loss: 0.0366 - val_accuracy: 0.9920\n",
      "Epoch 22/50\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.0104 - accuracy: 0.9964 - val_loss: 0.0360 - val_accuracy: 0.9926\n",
      "Epoch 23/50\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.0094 - accuracy: 0.9970 - val_loss: 0.0320 - val_accuracy: 0.9933\n",
      "Epoch 24/50\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.0080 - accuracy: 0.9972 - val_loss: 0.0326 - val_accuracy: 0.9929\n",
      "Epoch 25/50\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.0094 - accuracy: 0.9968 - val_loss: 0.0320 - val_accuracy: 0.9929\n",
      "Epoch 26/50\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.0079 - accuracy: 0.9974 - val_loss: 0.0330 - val_accuracy: 0.9924\n",
      "Epoch 27/50\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.0073 - accuracy: 0.9975 - val_loss: 0.0332 - val_accuracy: 0.9937\n",
      "Epoch 28/50\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.0075 - accuracy: 0.9974 - val_loss: 0.0422 - val_accuracy: 0.9912\n",
      "Epoch 29/50\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.0080 - accuracy: 0.9973 - val_loss: 0.0350 - val_accuracy: 0.9923\n",
      "Epoch 30/50\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.0063 - accuracy: 0.9977 - val_loss: 0.0410 - val_accuracy: 0.9925\n",
      "Epoch 31/50\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.0078 - accuracy: 0.9975 - val_loss: 0.0333 - val_accuracy: 0.9927\n",
      "Epoch 32/50\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.0069 - accuracy: 0.9977 - val_loss: 0.0357 - val_accuracy: 0.9929\n",
      "Epoch 33/50\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.0087 - accuracy: 0.9970 - val_loss: 0.0383 - val_accuracy: 0.9925\n",
      "Epoch 34/50\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.0076 - accuracy: 0.9975 - val_loss: 0.0342 - val_accuracy: 0.9929\n",
      "Epoch 35/50\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.0077 - accuracy: 0.9973 - val_loss: 0.0367 - val_accuracy: 0.9932\n",
      "Epoch 36/50\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.0063 - accuracy: 0.9979 - val_loss: 0.0393 - val_accuracy: 0.9928\n",
      "Epoch 37/50\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.0062 - accuracy: 0.9981 - val_loss: 0.0385 - val_accuracy: 0.9933\n",
      "Epoch 38/50\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.0069 - accuracy: 0.9976 - val_loss: 0.0319 - val_accuracy: 0.9940\n",
      "Epoch 39/50\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.0057 - accuracy: 0.9980 - val_loss: 0.0389 - val_accuracy: 0.9931\n",
      "Epoch 40/50\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.0055 - accuracy: 0.9981 - val_loss: 0.0400 - val_accuracy: 0.9927\n",
      "Epoch 41/50\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.0053 - accuracy: 0.9980 - val_loss: 0.0400 - val_accuracy: 0.9928\n",
      "Epoch 42/50\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.0050 - accuracy: 0.9981 - val_loss: 0.0397 - val_accuracy: 0.9926\n",
      "Epoch 43/50\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.0064 - accuracy: 0.9980 - val_loss: 0.0414 - val_accuracy: 0.9927\n",
      "Epoch 44/50\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.0050 - accuracy: 0.9984 - val_loss: 0.0435 - val_accuracy: 0.9927\n",
      "Epoch 45/50\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.0048 - accuracy: 0.9987 - val_loss: 0.0441 - val_accuracy: 0.9927\n",
      "Epoch 46/50\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.0056 - accuracy: 0.9982 - val_loss: 0.0410 - val_accuracy: 0.9933\n",
      "Epoch 47/50\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.0062 - accuracy: 0.9979 - val_loss: 0.0437 - val_accuracy: 0.9925\n",
      "Epoch 48/50\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.0056 - accuracy: 0.9981 - val_loss: 0.0370 - val_accuracy: 0.9923\n",
      "Epoch 49/50\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.0049 - accuracy: 0.9982 - val_loss: 0.0432 - val_accuracy: 0.9933\n",
      "Epoch 50/50\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.0052 - accuracy: 0.9980 - val_loss: 0.0432 - val_accuracy: 0.9933\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "epochs = 50\n",
    "history = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0327 - accuracy: 0.9938\n",
      "Test loss: 0.03266390413045883\n",
      "Test accuracy: 0.9937999844551086\n"
     ]
    }
   ],
   "source": [
    "# Final Result - Generalization!\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1)  \n",
    "  \n",
    "# summarize history for accuracy  \n",
    "   \n",
    "plt.subplot(211)  \n",
    "plt.plot(history.history['accuracy'])  \n",
    "plt.plot(history.history['val_accuracy'])  \n",
    "plt.title('model accuracy')  \n",
    "plt.ylabel('accuracy')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'test'], loc='upper left')  \n",
    "   \n",
    "# summarize history for loss  \n",
    "   \n",
    "plt.subplot(212)  \n",
    "plt.plot(history.history['loss'])  \n",
    "plt.plot(history.history['val_loss'])  \n",
    "plt.title('model loss')  \n",
    "plt.ylabel('loss')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'test'], loc='upper left') \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining the Weights\n",
    "\n",
    "Let's see if we can get a glimpse into what the network has learned by unpacking some of the weights in the network and plotting them. Since the kernel patches have a 2D structure to them, we would expect the network to start learning interesting 2D features which capture edges and common shapes in these small patches that can be stitched together to form composite digits. These features are what a convolution network is all about: changing the inductive bias of the network to better capture the kinds of features useful for classifying 2D/image data.\n",
    "\n",
    "OK, so we can use the `get_weights()` function to grab them, and we expect 8 sets of them. This is because 4 layers in the network construction introduced connection weights (only four rows in the summary have a non-zero entry in #Params). Also, connection weights and bias weights for each layer are divided into two separate tensors each (4 x 2 = 8). The connection weights will not be matrices since the addition of 2D structure rearranges them based on the selected kernel size.\n",
    "\n",
    "Let's grab one set, and then many more..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is everything!\n",
    "weights = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many tensors?\n",
    "len(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's the shape of the tensor\n",
    "# for the connection weights at\n",
    "# the -first- Conv2D layer?\n",
    "weights[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the filters are 8 x 8 x 1 (64 filters total since there are 64 units in that layer). We can slice off the dimension of size 1 by just grabbing index 0 along that dimension. Slicing along the dimension of size 64 will get an individual filter: for now, lets just look at the first one and see what we find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can plot the kernel weight matrix\n",
    "# for just the first unit (out of 64)\n",
    "# as an example...\n",
    "plt.imshow(weights[0][:,:,0,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The light values indicate high weights, and the dark regions indicate low weights. In general, high weights cause a unit to turn on when pixel intensities are high and low weights cause a unit to turn off when pixel intensities are high. Therefore regions which transition from high to low weights are finding edges. Different units will detect different shape and orienations of these edge features, which form the basic building blocks for object detection and recognition. We can look at all 64 of the units by creating a plot with 64 small 8 x 8 subplots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(8,8)\n",
    "fig.set_figwidth(12)\n",
    "fig.set_figheight(12)\n",
    "for x in range(8):\n",
    "    for y in range(8):\n",
    "        data = weights[0][:,:,:,(x*8)+y].reshape(8,8)\n",
    "        \n",
    "        axes[x,y].imshow(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These edge features are  then combined at the next `Conv2D` layer to form more complex features, so they are detecting changes in observed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(8,8)\n",
    "fig.set_figwidth(12)\n",
    "fig.set_figheight(12)\n",
    "for x in range(8):\n",
    "    for y in range(8):\n",
    "        axes[x,y].imshow(weights[2][:,:,(x*8)+y,0].reshape(8,8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Network Activations\n",
    "\n",
    "While looking at the weights can provide some insights into the function of these filters/kernels/\"feature detectors\", it doesn't show how the weights are used for any *particular* image. Therefore, it can be hard to see how these weights are *used* by the network when classifying a digit. In order to gather additional insight on what the network might have learned, we can instead send a pattern through the network and look at the *activation values* produced by the units at each layer. We would expect the feature detectors to turn on in different ways based on the different digits which are presented, essentially based on the features which are present or not present in a given image.\n",
    "\n",
    "Too start with, let's pass a single image (the first one in the x_train data set) through the network and extract the activations of the neurons for all layers..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the input layer/tensor of the model\n",
    "inp = model.input\n",
    "\n",
    "# Extract the output layer/tensors of the model\n",
    "# Note: there is one output tensor for each layer\n",
    "#       which contains the activation values\n",
    "#       for the layer in question...\n",
    "outputs = [layer.output for layer in model.layers]\n",
    "\n",
    "# Create a function to obtain the outputs given\n",
    "# an input tensor...\n",
    "functor = K.function(inp, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the layer outputs for the -first- image...\n",
    "layer_outputs = functor([x_train[0:1,:,:,:],1.])\n",
    "len(layer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have passed the first image through the network, and have extracted the activations for the 8 layers. Now we can visualize those activations to see what these feature detectors might be finding.\n",
    "\n",
    "First, we look at the initial convolutional layer of the network..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_outputs[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the dimensions of the output tensor for the first layer, we can observe that there are 64 units each with a 1 x 21 x 21 output tensor. The 1 is due to the fact that we only have one channel (intensity). The 21 x 21 corresponds to the sliding of the 8 x 8 filters across the 28 x 28 pixel input images. Let's plot each of those 21 x 21 activation patterns to see if there is anything interesting there..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(8,8)\n",
    "fig.set_figwidth(12)\n",
    "fig.set_figheight(12)\n",
    "for x in range(8):\n",
    "    for y in range(8):\n",
    "        axes[x,y].imshow(layer_outputs[0][0,:,:,(x*8)+y])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be pretty clear that these units are detecting simple features for the number 5, which is indeed the first image in the x_train set. You might notice that some of the neural units seem to be finding edges or boundaries. Some units find edges or other features on just one side, some on both sides, and others seem to detect empty or surrounding regions which *lack* pixels.\n",
    "\n",
    "What would we find in the second layer? This one is a little more complicated since it's composites of features and there are 128 units. Still, we can try the same approach..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_outputs[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(12,11)\n",
    "fig.set_figwidth(12)\n",
    "fig.set_figheight(12)\n",
    "for x in range(128):\n",
    "    axes[int(x/11),int(x%11)].imshow(layer_outputs[1][0,:,:,x])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such composite features are somewhat difficult to understand without sampling from a wide range of images, but these are basically combinations of features that are somewhat unique to the number 5. Of course some of them will be active for numbers that look similar in certain ways, but the key *differences* will also be observed in these activation patterns.\n",
    "\n",
    "Given that the downstream layers should start to look less like composite features, but unique vectors for describing each image, we can now try to visualize those vectors using PCA..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many of the training images to use?\n",
    "n_train = 1000\n",
    "\n",
    "# Extract the outputs for -all- requested patterns\n",
    "layer_outputs = [functor([test[np.newaxis,...],1.]) for test in x_train[0:n_train,:,:,:]]\n",
    "\n",
    "# Extract just the activations of the -sixth- layer in the network.\n",
    "# Remember, this is a -dense- layer with 128 neural units.\n",
    "# We can think of this as a 128-dimensional space in which each \n",
    "# of the patterns has now been embedded by the neural network...\n",
    "A = np.vstack([out[5].flatten() for out in layer_outputs])\n",
    "\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix, $\\boldsymbol{A}$, now contains the activations of the 6th layer of the network (dense, 128 units) for the first 1000 training patterns. Of course, we can't visualize a 128-dimensional space, but using PCA we can project this down to a smaller number of dimensions and see if we can observe any trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean center a vector\n",
    "def mean_center(x):\n",
    "    return x - np.mean(x)\n",
    "\n",
    "# Call this function for each column in the data (move along axis 0 or the rows)\n",
    "Acentered = np.apply_along_axis(mean_center,0,A)\n",
    "\n",
    "U,S,V = np.linalg.svd(Acentered,full_matrices=True)\n",
    "\n",
    "# Percent variance accounted for\n",
    "plt.plot(100.0*S/np.sum(S))\n",
    "plt.ylabel('% Var')\n",
    "plt.xlabel('Singular Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scree plot above shows that it still takes a large number of dimensions to embed the data. However, maybe a 3D projection will show a decent amount of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100.0*np.sum(S[0:3])/np.sum(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 3 principal components will only capture about 19% of the variance in the data, but let's color the plot by the various classes and see if it looks better than the PCA of the original data which we saw back in Open Lab 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.zeros([A.shape[0],A.shape[1]])\n",
    "np.fill_diagonal(D,S)\n",
    "Arotated = np.dot(U,D)\n",
    "\n",
    "# First three principal components!\n",
    "PCs = Arotated[:,0:3]\n",
    "PCs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need a lot of colors again (10 - one for each class)\n",
    "# PCs 1 and 2\n",
    "plt.scatter(PCs[:,0],PCs[:,1],\n",
    "            color=[['red','green','blue','cyan','magenta','yellow','black','brown','grey','purple'][i] for i in np.apply_along_axis(np.argmax,1,y_train[0:n_train,:])])\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCs 2 and 3\n",
    "plt.scatter(PCs[:,2],PCs[:,1],\n",
    "            color=[['red','green','blue','cyan','magenta','yellow','black','brown','grey','purple'][i] for i in np.apply_along_axis(np.argmax,1,y_train[0:n_train,:])])\n",
    "plt.xlabel(\"PC3\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two plots above utilize 2 separate 2D plots to attempt to visualize the first 3 principal components. The first shows PCs 1 and 2 while the second shows PCs 2 and 3. Therefore, they share PC 2 which can help show where one plot corresponds to the other. Also, if some groups are overlapping in one plot, they are usually not in the other (or they do not overlap in either plot). Therefore, it looks like the 128-dimensional embedding created at the downstream dense layer is providing a good space in which decision boundaries (hyper-planes) between the different classes might easily be drawn.\n",
    "\n",
    "Using a 3D plot can make this a little clearer, but it's difficult to render just the right angle for viewing all of the point clouds. Let's render at a couple of different angles to see if that helps..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D Plot\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "fig.set_figwidth(10)\n",
    "fig.set_figheight(8)\n",
    "ax.scatter(PCs[:,0],PCs[:,1],PCs[:,2],\n",
    "           color=[['red','green','blue','cyan','magenta','yellow','black','brown','grey','purple'][i] for i in np.apply_along_axis(np.argmax,1,y_train[0:n_train,:])])\n",
    "ax.set_xlabel(\"PC1\")\n",
    "ax.set_ylabel(\"PC2\")\n",
    "ax.set_zlabel(\"PC3\")\n",
    "ax.view_init(45,45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D Plot\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "fig.set_figwidth(10)\n",
    "fig.set_figheight(8)\n",
    "ax.scatter(PCs[:,0],PCs[:,1],PCs[:,2],\n",
    "           color=[['red','green','blue','cyan','magenta','yellow','black','brown','grey','purple'][i] for i in np.apply_along_axis(np.argmax,1,y_train[0:n_train,:])])\n",
    "ax.set_xlabel(\"PC1\")\n",
    "ax.set_ylabel(\"PC2\")\n",
    "ax.set_zlabel(\"PC3\")\n",
    "ax.view_init(45,225)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This PCA projection shows that the 10 classes are fairly well separated, and that's using a projection which only accounts for 19% of the total variance in the data. Even still, it is clear that the network has learned to separate the different images into unique regions of the neural activations space. This allows the final output layer to form a clear decision boundary between the different image classes and achieve *very good* (almost state-of-the-art) performance with just a handful of training epochs. That's a pretty helpful inductive bias, no?\n",
    "\n",
    "In the end, the general approach to plotting weights may help us to understand what's happening for *all* patterns, but analyzing activations can provide additional insights since they change and vary with the input data itself. Still, this often requires some additional post-processing which may be prohibitively expensive. For example, we could have tried the same approach with the activations of the convolution layers, but the size of the activations matrices is much larger than the later downstream layers. This would make the PCA prohibitively expensive to perform. Later in the semester, we will look at a way to train a neural network to solve PCA which will require epochs of training but is much more memory efficient than the SVD approach used above.\n",
    "\n",
    "We've now built and trained a basic convolution neural network, but there's more on this to come soon!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Pre-trained Model (Transfer Learning)\n",
    "\n",
    "Now that we have explored building a ConvNet from scratch using MNIST, let's explore two more potentially useful techniques:\n",
    "1. 3-channel (R,G,B) images\n",
    "2. Pre-trained ConvNet for early stages of processing\n",
    "\n",
    "We will utilize full-color images (increasing the third dimension of our input tensors to 3). Images will be of a different size now as well since we will build the convolution layers of our network using a *pretrained* set of layers and connection weights which have already been trained on an extensive set of images. This is useful since we don't necessary have the time and memory resources to burn on training up the low-level features useful for generalized classification. Instead, we will exploit the pre-trained network topology and then stack some additional layers on top of these pre-trained layers to perform our *particular* categorization task. This should also allow for faster training of the network since updating the connection weights on the earlier layers of the network should be minimal. This reduction in training epochs for training those deep weights in the network will significantly reduce our training time on our image classification task of interest.\n",
    "\n",
    "First though, we need to load up an image data set to use. This example will consist of 100 images borrowed from [ImageNet](http://www.image-net.org). I've already prepared the data set on the JupyterLab systems for this course in the shared directory `/nfshome/sandbox/xception_example/`. You can also download the data set from the JupyterLab systems (just use the terminal to create a link to the sandbox while in your home directory: `ln -s /nfshome/sandbox` on any of the systems, browse into the newly created `sandbox` folder link and right click-download `exception-example.zip`). You will need to adjust some of the paths below if you decide to download the data yourself. Due to copyright restrictions, I am not going to post this data set on the course website, and some of the image processing tools that can read images from URLs directly were also performing some additional processing of the images that might not be compatible with this tutorial, so we won't use them until that gets fixed.\n",
    "\n",
    "OK, let's load up some images and see what we have..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First a few tools for this -particular- example...\n",
    "from keras.applications.xception import Xception\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.xception import preprocess_input, decode_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_image(img_path):\n",
    "    img = image.load_img(img_path, target_size=(299, 299))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    return x\n",
    "\n",
    "X = np.concatenate([grab_image('/nfshome/sandbox/xception_example/images/image_%d.JPEG'%(i)) for i in range(100)])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will note that this is a *4D tensor* of input data for the network. The first dimension is 100 since there are 100 images, each image is 299 x 299 pixels (dims 2 and 3), and the last dimension is of size 3 for the three color channels of the image (red, green, blue). After the above code has been run, each image has already been preprocessed for input into the pre-trained network that we will be using: Googles's Xception net.\n",
    "\n",
    "If you want to verify any of the images though, you can reconvert the preprocessed arrays *back* into an image format accepted by matplotlib, and use `imshow()` to display it.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image.array_to_img(X[0,:,:,:]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a parakeet, and the next is a dog hanging out with some sheep!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image.array_to_img(X[1,:,:,:]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will load up a pre-trained model (Google's Xception) which classifies any provided image (scaled to 299 x 299 pixels) into one of 1000 different categories (the output layer has 1000 units and a softmax activation function). This network is pretty large (but still small compared to some convnets) so get ready..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load up the model\n",
    "# This will load up the architecture\n",
    "# and then the pre-trained weights from the\n",
    "# internet...\n",
    "model = Xception(weights='imagenet')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a big network, no? Even though the network is very deep, the total number of parameters is only about 23 million, which is actually *very* compact and this is still an excellent network for image classification. Let's just use the default architecture and see what it says about the first image in our data set using the `predict()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just the first image that we say above...\n",
    "preds = model.predict(X[0:1,:,:,:])\n",
    "\n",
    "# Decode the results into a list of tuples (class, description, probability)\n",
    "# (one such list for each sample in the batch)\n",
    "print('Predicted:', decode_predictions(preds, top=5)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's pretty good. There are 1000 different categories used here, so lot's of different images will be categorized pretty will with at least one of the top 5 or so results. Here, the first hit at 44.8% is a very good fit even though it's not *really* an African Grey. Even the next 4 highest probabilities are all pretty close as well (never trained on this specific class anyway).\n",
    "\n",
    "OK, so now we can use the model to make a class prediction, but let's see if we can use the pretrained network and build our own classifier on *top* of this model. In order to do this, we will need some different class labels for our data. For these 100 images, 37 have pictures of cats. Therefore, we are going to make a simple two-class predictor (cat vs. no-cat), but use Xception as the first part of our network. Let's load the class labels, and build a net with an Xception front-end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 37 cats in there (label 1, and all else 0)\n",
    "Y = np.loadtxt('/nfshome/sandbox/xception_example/class_labels.csv')\n",
    "Y = keras.utils.to_categorical(Y)\n",
    "\n",
    "# Take a look at the first 10...\n",
    "Y[0:10,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh, so number 5 must be a cat picture, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image.array_to_img(X[4,:,:,:]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now the idea here is make a cat predictor, but **reuse** the pre-trained Xception network instead of starting from scratch. The basic intuition is that the low-level features extracted by Xception are very general since it was trained with a wide range of images (from 1000 different classes!). Most of those basic feature detectors should be useful for discriminating between cats and other things then as well, right?\n",
    "\n",
    "Well, let's see..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an Xception-based network\n",
    "\n",
    "# Base Model\n",
    "base_model = keras.applications.Xception(weights='imagenet',include_top=False)\n",
    "# This model uses the trained weights, but critically,\n",
    "# the include_top=False flag is provided to allow us to\n",
    "# add our own layers at the top...\n",
    "\n",
    "# New model that we are adding at the end...\n",
    "# We will start by setting the -start- of the model\n",
    "# to the -end- (output) of the pretrained model...\n",
    "new_model = base_model.output\n",
    "\n",
    "# Now, we can start adding our own layers to the model...\n",
    "# Here, I am using the -functional- network model for\n",
    "# Keras instead of the -sequential- model. This means we\n",
    "# need to \"manually\" tie our layers together by creating\n",
    "# them and then providing the upstream layer as an argument\n",
    "# to the object's functional interface:\n",
    "new_model = keras.layers.GlobalAveragePooling2D()(new_model)\n",
    "# We are using an average pooling layer to select the most\n",
    "# likely among the features provided by the base_network...\n",
    "\n",
    "# Now, let's continue to stack layers into the model...\n",
    "new_model = keras.layers.Dense(30,activation='relu')(new_model)\n",
    "\n",
    "# Add the final output layer (just 2 classes)\n",
    "new_model = keras.layers.Dense(2,activation='softmax')(new_model)\n",
    "\n",
    "# For the functional model, we first build up a set of layers\n",
    "# and network componenets, and then specify which of those\n",
    "# layers and components function as the input and output for\n",
    "# our model.\n",
    "model = keras.Model(inputs=base_model.input,outputs=new_model)\n",
    "# It's possible to have -multiple- input or output components\n",
    "# for a model as well, but here we just use the Xception model's\n",
    "# input layer and the output layer from our added layers as\n",
    "# the output layer for the mode...\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Take a quick look at it...\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the extra layers stacked at the end that we made! Now we are ready to try to train this model up. Remember, this is another *big* model, so the training time is great, but it should still be better than starting from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "epochs = 10\n",
    "history = model.fit(X, Y,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_split = 0.4)\n",
    "# 0.4 split size provides 25 cats in the training set,\n",
    "# and 12 in the validation set..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)  \n",
    "  \n",
    "# summarize history for accuracy  \n",
    "   \n",
    "plt.subplot(211)  \n",
    "plt.plot(history.history['acc'])  \n",
    "plt.plot(history.history['val_acc'])  \n",
    "plt.title('model accuracy')  \n",
    "plt.ylabel('accuracy')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'test'], loc='upper left')  \n",
    "   \n",
    "# summarize history for loss  \n",
    "   \n",
    "plt.subplot(212)  \n",
    "plt.plot(history.history['loss'])  \n",
    "plt.plot(history.history['val_loss'])  \n",
    "plt.title('model loss')  \n",
    "plt.ylabel('loss')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'test'], loc='upper left') \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, we ended up with a *horrible* case of overfitting since the performance on the test cases is so bad, but we can at least get it to learn the training data (100%). We could, of course, add *dropout* or some other regularization method to try to cope with this, but we know for a fact that 60 images is a little **data starved** for this kind of task. So instead, let's just train up a conv net from scratch for 10 epochs for comparison, and see what it can do in that same amount of effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A convolution net for the cat problem!\n",
    "model = keras.Sequential()\n",
    "# Three input channels this time!\n",
    "model.add(keras.layers.Conv2D(64, kernel_size=(8, 8),\n",
    "                              activation='relu',\n",
    "                              input_shape=[X.shape[1],\n",
    "                                           X.shape[2],\n",
    "                                           X.shape[3]]))\n",
    "model.add(keras.layers.Conv2D(128, (8, 8), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(8, 8)))\n",
    "model.add(keras.layers.Dropout(0.25))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(128, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Dense(Y.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Roughly the same number of parameters!\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "epochs = 10\n",
    "history = model.fit(X, Y,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_split = 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)  \n",
    "  \n",
    "# summarize history for accuracy  \n",
    "   \n",
    "plt.subplot(211)  \n",
    "plt.plot(history.history['acc'])  \n",
    "plt.plot(history.history['val_acc'])  \n",
    "plt.title('model accuracy')  \n",
    "plt.ylabel('accuracy')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'test'], loc='upper left')  \n",
    "   \n",
    "# summarize history for loss  \n",
    "   \n",
    "plt.subplot(212)  \n",
    "plt.plot(history.history['loss'])  \n",
    "plt.plot(history.history['val_loss'])  \n",
    "plt.title('model loss')  \n",
    "plt.ylabel('loss')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'test'], loc='upper left') \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may take a while to train these large networks, so starting from the pre-trained network is a *huge* advantage in most cases. Don't let the 70% test accuracy fool you on the \"*from scratch*\" network just above: we haven't even begun to decrease in loss for the training data, so it's not an indication of future performance. Pre-trained networks (transfer learning) will likely become more and more common in future applications due to the large computational resources and careful tuning required for large-scale performance when training networks from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "\n",
    "1. Build a convolution neural network that solves the *cat identification* problem for the 100 image set explored above with *100% accuracy* on the training data.\n",
    "    * You should read the images in and *resize* them down to just 100 x 100 pixels for this problem.\n",
    "    * Your neural network should use a convolution neural network topology similar to the one used to solve the MNIST problem above.\n",
    "    * Feel free to change the kernel sizes and such to limit the size of the network for feasibility of training.\n",
    "    * You should *not* need a network with more than 10 million parameters.\n",
    "    * You should *not* use a validation set at all for this problem/assignment.\n",
    "2. Provide a plot of the learning history of the network from initial creation to 100% performance showing both *loss* and *accuracy* statistics.\n",
    "3. Create an Xception network and classify all 100 of the images from the *cat indentification* problem using the default  Xception categories.\n",
    "    * You should reload the image data as 299 x 299 pixel images for this problem.\n",
    "    * You should only print the top *one* class label for each image as decided by Xception.\n",
    "\n",
    "Compile your answers in a single IPython notebook file, zip it, and upload it to the course assignment submission system by the due date at the top of this assignment.\n",
    "\n",
    "Assignment Submission System: [LINK](https://4850.cs.mtsu.edu/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright &copy; 2019 Joshua L. Phillips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
